import numpy as np
import torch
from utils import float32_preprocessor

def test_net(net, env, count=3, device="cpu", act_only=False):
    rewards = 0.0
    steps = 0
    for _ in range(count):
        obs = env.reset()
        while True:
            obs_v = float32_preprocessor([obs]).to(device)
            if act_only:
                mu_v = net(obs_v) # may not need this
            else:
                mu_v = net(obs_v)[0]
            action = mu_v.squeeze(dim=0).data.cpu().numpy()
            action = np.clip(action, -1, 1)
            obs, reward, done, _ = env.step(action)
            rewards += reward
            steps += 1
            if done:
                break
    return rewards / count, steps / count


def calc_logprob(mu_v, var_v, actions_v):
    p1 = - ((mu_v - actions_v) ** 2) / (2*var_v.clamp(min=1e-3))
    p2 = - torch.log(torch.sqrt(2 * np.pi * var_v))
    return p1 + p2

# Implementation from book: seems incorrect
# def calc_logprob(mu_v, logstd_v, actions_v):
#     p1 = - ((mu_v - actions_v) ** 2) / (2*torch.exp(logstd_v).clamp(min=1e-3))
#     p2 = - torch.log(torch.sqrt(2 * np.pi * torch.exp(logstd_v)))
#     return p1 + p2


def unpack_batch_a2c(batch, net, last_val_gamma, device="cpu", seperate_act_crt=False):
    states, actions, rewards, not_done_idx, last_states = [], [], [], [], []
    for idx, exp in enumerate(batch):
        states.append(np.array(exp.state, copy=False))
        actions.append(exp.action)
        rewards.append(exp.reward)
        if exp.last_state is not None:
            not_done_idx.append(idx)
            last_states.append(np.array(exp.last_state, copy=False))
    #states_v = torch.FloatTensor(np.array(states, copy=False)).to(device)
    states_v = float32_preprocessor(states).to(device)
    actions_v = torch.FloatTensor(actions).to(device)
    
    rewards_np = np.array(rewards, dtype=np.float32) # n-step reward
    if not_done_idx:
        last_states_v = float32_preprocessor(last_states).to(device)
        if seperate_act_crt:
            last_vals_v = net(last_states_v)
        else:
            last_vals_v = net(last_states_v)[2]
        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]
        rewards_np[not_done_idx] += last_val_gamma * last_vals_np

    ref_vals_v = torch.FloatTensor(rewards_np).to(device)
    return states_v, actions_v, ref_vals_v


def unpack_batch_ddqn(batch, device="cpu"):
    states, actions, rewards, dones, last_states = [], [], [], [], []
    for exp in batch:
        states.append(np.array(exp.state, copy=False))
        actions.append(exp.action)
        rewards.append(exp.reward)
        dones.append(exp.last_state is None)
        if exp.last_state is None:
            last_states.append(exp.state) # will be masked
        else:
            last_states.append(exp.last_state)
    states_v = float32_preprocessor(states).to(device)
    actions_v = float32_preprocessor(actions).to(device)
    rewards_v = float32_preprocessor(rewards).to(device)
    last_states_v = float32_preprocessor(last_states).to(device)
    dones_t = torch.BoolTensor(dones).to(device)
    return states_v, actions_v, rewards_v, dones_t, last_states_v