{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gym\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import experience, utils, common\n",
    "from model import Actor_Discrete, Critic, AgentA2C_Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"MountainCar-v0\" ; STOP_REWARD = -120\n",
    "#ENV_NAME = \"CartPole-v0\"; STOP_REWARD = 190\n",
    "#ENV_NAME = 'LunarLander-v2'; STOP_REWARD = 190\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 0.0005 \n",
    "LR_RATIO = 5  # crt_lr / act_lr\n",
    "TEST_ITERS = 50_000\n",
    "MAX_STEPS = 5_000_000\n",
    "\n",
    "GAE_LAMBDA = 0.95\n",
    "TRAJECTORY_SIZE =  256 \n",
    "BATCH_SIZE = 64 //2 \n",
    "PPO_EPS = 0.2\n",
    "PPO_EPOCHES = 4\n",
    "ENTROPY_BONUS = 0.001 \n",
    "HID_SIZE = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adv_ref(trajectory, net_crt, states_v, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    By trajectory calculate advantage and 1-step ref value\n",
    "    :param trajectory: trajectory list\n",
    "    :param net_crt: critic network\n",
    "    :param states_v: states tensor\n",
    "    :return: tuple with advantage numpy array and reference values\n",
    "    \"\"\"\n",
    "    values_v = net_crt(states_v)\n",
    "    values = values_v.squeeze().data.cpu().numpy()\n",
    "    last_gae = 0.0\n",
    "    result_adv = []\n",
    "    result_ref = []\n",
    "    for val, next_val, (exp,) in zip(\n",
    "        reversed(values[:-1]), reversed(values[1:]), reversed(trajectory[:-1])\n",
    "    ):\n",
    "        if exp.done:\n",
    "            delta = exp.reward - val\n",
    "            last_gae = delta\n",
    "        else:\n",
    "            delta = exp.reward + GAMMA * next_val - val\n",
    "            last_gae = delta + GAMMA * GAE_LAMBDA * last_gae\n",
    "        result_adv.append(last_gae)\n",
    "        result_ref.append(last_gae + val)  # advantage + value = q_value\n",
    "\n",
    "    adv_v = torch.FloatTensor(list(reversed(result_adv)))\n",
    "    ref_v = torch.FloatTensor(list(reversed(result_ref)))\n",
    "    return adv_v.to(device), ref_v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "save_path = os.path.join(\"saves\", \"ppo-\" + f\"{ENV_NAME}\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "env = gym.make(ENV_NAME)\n",
    "test_env = gym.make(ENV_NAME)\n",
    "obs_size = test_env.observation_space.shape[0]\n",
    "act_size = test_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Discrete(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(Actor_Discrete, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, HID_SIZE),\n",
    "            nn.ReLU(), \n",
    "            #nn.Linear(HID_SIZE, HID_SIZE),\n",
    "            #nn.ReLU(), \n",
    "            nn.Linear(HID_SIZE, act_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_size):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(obs_size, HID_SIZE),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(HID_SIZE, HID_SIZE),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(HID_SIZE, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.value(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4447, 0.1329, 0.9013],\n",
       "        [0.0490, 0.4662, 0.2285],\n",
       "        [0.1484, 0.5669, 0.3991],\n",
       "        [0.2055, 0.5994, 0.1501],\n",
       "        [0.5368, 0.0106, 0.2218],\n",
       "        [0.0261, 0.1805, 0.0350]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.rand((6,3))\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = Categorical(probs)\n",
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_act = Actor_Discrete(obs_size, act_size).to(device)\n",
    "net_crt = Critic(obs_size).to(device)\n",
    "agent = AgentA2C_Discrete(net_act, device=device)\n",
    "exp_source = experience.ExperienceSource(env, agent, steps_count=1)\n",
    "opt_crt = optim.Adam(net_crt.parameters(), lr=LR)\n",
    "opt_act = optim.Adam(net_act.parameters(), lr=LR / LR_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\n",
    "        comment=\"-ppo_\"\n",
    "        + f\"{ENV_NAME}-L{LR}R{LR_RATIO}_T{TRAJECTORY_SIZE}\"\n",
    "        + f\"B{BATCH_SIZE}_E{PPO_EPOCHES}\" \n",
    "    )\n",
    "\n",
    "trajectory = []\n",
    "best_reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test done is 0.15 sec, reward -200.00, steps 200\n",
      "400: done 2 episodes, mean reward -200.000, speed 343.40 f/s\n",
      "2200: done 11 episodes, mean reward -200.000, speed 1758.65 f/s\n",
      "4200: done 21 episodes, mean reward -200.000, speed 1785.30 f/s\n",
      "6200: done 31 episodes, mean reward -200.000, speed 1852.71 f/s\n",
      "8200: done 41 episodes, mean reward -200.000, speed 1987.37 f/s\n",
      "10200: done 51 episodes, mean reward -200.000, speed 1878.65 f/s\n",
      "12200: done 61 episodes, mean reward -200.000, speed 1853.14 f/s\n",
      "14200: done 71 episodes, mean reward -200.000, speed 1887.20 f/s\n",
      "16200: done 81 episodes, mean reward -200.000, speed 1833.11 f/s\n",
      "18200: done 91 episodes, mean reward -200.000, speed 1924.99 f/s\n",
      "20200: done 101 episodes, mean reward -200.000, speed 1909.15 f/s\n",
      "22200: done 111 episodes, mean reward -200.000, speed 1866.08 f/s\n",
      "24200: done 121 episodes, mean reward -200.000, speed 1886.69 f/s\n",
      "26200: done 131 episodes, mean reward -200.000, speed 1935.21 f/s\n",
      "28200: done 141 episodes, mean reward -200.000, speed 1902.25 f/s\n",
      "30200: done 151 episodes, mean reward -200.000, speed 1879.69 f/s\n",
      "32200: done 161 episodes, mean reward -200.000, speed 1862.04 f/s\n",
      "34200: done 171 episodes, mean reward -200.000, speed 1880.30 f/s\n",
      "36200: done 181 episodes, mean reward -200.000, speed 1968.56 f/s\n",
      "38200: done 191 episodes, mean reward -200.000, speed 1881.39 f/s\n",
      "40200: done 201 episodes, mean reward -200.000, speed 1890.22 f/s\n",
      "42200: done 211 episodes, mean reward -200.000, speed 1885.14 f/s\n",
      "44200: done 221 episodes, mean reward -200.000, speed 1964.06 f/s\n",
      "46200: done 231 episodes, mean reward -200.000, speed 1854.95 f/s\n",
      "48200: done 241 episodes, mean reward -200.000, speed 1842.83 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "50200: done 251 episodes, mean reward -200.000, speed 1587.09 f/s\n",
      "52200: done 261 episodes, mean reward -200.000, speed 1931.98 f/s\n",
      "54200: done 271 episodes, mean reward -200.000, speed 1984.12 f/s\n",
      "56200: done 281 episodes, mean reward -200.000, speed 1850.18 f/s\n",
      "58200: done 291 episodes, mean reward -200.000, speed 1848.25 f/s\n",
      "60200: done 301 episodes, mean reward -200.000, speed 1861.39 f/s\n",
      "62200: done 311 episodes, mean reward -200.000, speed 1847.37 f/s\n",
      "64400: done 322 episodes, mean reward -200.000, speed 1963.82 f/s\n",
      "66400: done 332 episodes, mean reward -200.000, speed 1889.14 f/s\n",
      "68400: done 342 episodes, mean reward -200.000, speed 1861.09 f/s\n",
      "70400: done 352 episodes, mean reward -200.000, speed 1963.06 f/s\n",
      "72400: done 362 episodes, mean reward -200.000, speed 1867.18 f/s\n",
      "74400: done 372 episodes, mean reward -200.000, speed 1898.63 f/s\n",
      "76400: done 382 episodes, mean reward -200.000, speed 1878.64 f/s\n",
      "78400: done 392 episodes, mean reward -200.000, speed 1839.17 f/s\n",
      "80600: done 403 episodes, mean reward -200.000, speed 1983.71 f/s\n",
      "82600: done 413 episodes, mean reward -200.000, speed 1846.25 f/s\n",
      "84600: done 423 episodes, mean reward -200.000, speed 1868.87 f/s\n",
      "86600: done 433 episodes, mean reward -200.000, speed 1994.48 f/s\n",
      "88600: done 443 episodes, mean reward -200.000, speed 1873.18 f/s\n",
      "90600: done 453 episodes, mean reward -200.000, speed 1854.31 f/s\n",
      "92600: done 463 episodes, mean reward -200.000, speed 1875.05 f/s\n",
      "94600: done 473 episodes, mean reward -200.000, speed 1849.99 f/s\n",
      "96600: done 483 episodes, mean reward -200.000, speed 1992.02 f/s\n",
      "98600: done 493 episodes, mean reward -200.000, speed 1825.21 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "100400: done 502 episodes, mean reward -200.000, speed 1637.23 f/s\n",
      "102400: done 512 episodes, mean reward -200.000, speed 1898.47 f/s\n",
      "104400: done 522 episodes, mean reward -200.000, speed 1906.10 f/s\n",
      "106400: done 532 episodes, mean reward -200.000, speed 1866.58 f/s\n",
      "108400: done 542 episodes, mean reward -200.000, speed 1968.27 f/s\n",
      "110400: done 552 episodes, mean reward -200.000, speed 1882.31 f/s\n",
      "112400: done 562 episodes, mean reward -200.000, speed 1900.66 f/s\n",
      "114400: done 572 episodes, mean reward -200.000, speed 1851.11 f/s\n",
      "116400: done 582 episodes, mean reward -200.000, speed 1973.74 f/s\n",
      "118400: done 592 episodes, mean reward -200.000, speed 1840.52 f/s\n",
      "120400: done 602 episodes, mean reward -200.000, speed 1876.44 f/s\n",
      "122400: done 612 episodes, mean reward -200.000, speed 1872.84 f/s\n",
      "124400: done 622 episodes, mean reward -200.000, speed 1876.94 f/s\n",
      "126400: done 632 episodes, mean reward -200.000, speed 1944.78 f/s\n",
      "128400: done 642 episodes, mean reward -200.000, speed 1868.50 f/s\n",
      "130400: done 652 episodes, mean reward -200.000, speed 1899.24 f/s\n",
      "132400: done 662 episodes, mean reward -200.000, speed 1878.09 f/s\n",
      "134400: done 672 episodes, mean reward -200.000, speed 1982.11 f/s\n",
      "136400: done 682 episodes, mean reward -200.000, speed 1859.96 f/s\n",
      "138400: done 692 episodes, mean reward -200.000, speed 1875.29 f/s\n",
      "140400: done 702 episodes, mean reward -200.000, speed 1853.78 f/s\n",
      "142400: done 712 episodes, mean reward -200.000, speed 1872.18 f/s\n",
      "144200: done 721 episodes, mean reward -200.000, speed 1750.52 f/s\n",
      "146200: done 731 episodes, mean reward -200.000, speed 1946.80 f/s\n",
      "148200: done 741 episodes, mean reward -200.000, speed 1907.15 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "150200: done 751 episodes, mean reward -200.000, speed 1639.95 f/s\n",
      "152200: done 761 episodes, mean reward -200.000, speed 1877.64 f/s\n",
      "154200: done 771 episodes, mean reward -200.000, speed 1890.14 f/s\n",
      "156400: done 782 episodes, mean reward -200.000, speed 1995.31 f/s\n",
      "158400: done 792 episodes, mean reward -200.000, speed 1825.87 f/s\n",
      "160400: done 802 episodes, mean reward -200.000, speed 1826.76 f/s\n",
      "162400: done 812 episodes, mean reward -200.000, speed 1896.36 f/s\n",
      "164400: done 822 episodes, mean reward -200.000, speed 1871.73 f/s\n",
      "166400: done 832 episodes, mean reward -200.000, speed 1892.70 f/s\n",
      "168400: done 842 episodes, mean reward -200.000, speed 1820.42 f/s\n",
      "170400: done 852 episodes, mean reward -200.000, speed 1871.86 f/s\n",
      "172400: done 862 episodes, mean reward -200.000, speed 1977.00 f/s\n",
      "174400: done 872 episodes, mean reward -200.000, speed 1868.41 f/s\n",
      "176400: done 882 episodes, mean reward -200.000, speed 1848.56 f/s\n",
      "178400: done 892 episodes, mean reward -200.000, speed 1900.48 f/s\n",
      "180400: done 902 episodes, mean reward -200.000, speed 1989.21 f/s\n",
      "182400: done 912 episodes, mean reward -200.000, speed 1902.95 f/s\n",
      "184200: done 921 episodes, mean reward -200.000, speed 1753.78 f/s\n",
      "186000: done 930 episodes, mean reward -200.000, speed 1722.93 f/s\n",
      "187800: done 939 episodes, mean reward -200.000, speed 1684.10 f/s\n",
      "189600: done 948 episodes, mean reward -200.000, speed 1733.07 f/s\n",
      "191400: done 957 episodes, mean reward -200.000, speed 1784.91 f/s\n",
      "193400: done 967 episodes, mean reward -200.000, speed 1811.79 f/s\n",
      "195200: done 976 episodes, mean reward -200.000, speed 1761.72 f/s\n",
      "197200: done 986 episodes, mean reward -200.000, speed 1823.11 f/s\n",
      "199200: done 996 episodes, mean reward -200.000, speed 1853.14 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "201000: done 1005 episodes, mean reward -200.000, speed 1609.91 f/s\n",
      "203000: done 1015 episodes, mean reward -200.000, speed 1953.95 f/s\n",
      "205000: done 1025 episodes, mean reward -200.000, speed 1854.90 f/s\n",
      "207000: done 1035 episodes, mean reward -200.000, speed 1926.00 f/s\n",
      "209000: done 1045 episodes, mean reward -200.000, speed 1872.73 f/s\n",
      "211000: done 1055 episodes, mean reward -200.000, speed 1884.03 f/s\n",
      "213200: done 1066 episodes, mean reward -200.000, speed 1956.86 f/s\n",
      "215200: done 1076 episodes, mean reward -200.000, speed 1860.97 f/s\n",
      "217200: done 1086 episodes, mean reward -200.000, speed 1847.93 f/s\n",
      "219400: done 1097 episodes, mean reward -200.000, speed 1959.99 f/s\n",
      "221400: done 1107 episodes, mean reward -200.000, speed 1855.42 f/s\n",
      "223400: done 1117 episodes, mean reward -200.000, speed 1900.75 f/s\n",
      "225400: done 1127 episodes, mean reward -200.000, speed 1816.44 f/s\n",
      "227200: done 1136 episodes, mean reward -200.000, speed 1765.44 f/s\n",
      "229000: done 1145 episodes, mean reward -200.000, speed 1795.95 f/s\n",
      "231000: done 1155 episodes, mean reward -200.000, speed 1916.50 f/s\n",
      "232800: done 1164 episodes, mean reward -200.000, speed 1793.95 f/s\n",
      "234600: done 1173 episodes, mean reward -200.000, speed 1799.13 f/s\n",
      "236600: done 1183 episodes, mean reward -200.000, speed 1846.46 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238600: done 1193 episodes, mean reward -200.000, speed 1820.24 f/s\n",
      "240600: done 1203 episodes, mean reward -200.000, speed 1840.86 f/s\n",
      "242400: done 1212 episodes, mean reward -200.000, speed 1774.82 f/s\n",
      "244400: done 1222 episodes, mean reward -200.000, speed 1937.13 f/s\n",
      "246400: done 1232 episodes, mean reward -200.000, speed 1815.79 f/s\n",
      "248400: done 1242 episodes, mean reward -200.000, speed 1868.30 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "250200: done 1251 episodes, mean reward -200.000, speed 1629.14 f/s\n",
      "252200: done 1261 episodes, mean reward -200.000, speed 1867.96 f/s\n",
      "254200: done 1271 episodes, mean reward -200.000, speed 1847.84 f/s\n",
      "256200: done 1281 episodes, mean reward -200.000, speed 1980.86 f/s\n",
      "258200: done 1291 episodes, mean reward -200.000, speed 1860.62 f/s\n",
      "260200: done 1301 episodes, mean reward -200.000, speed 1877.15 f/s\n",
      "262200: done 1311 episodes, mean reward -200.000, speed 1846.17 f/s\n",
      "264200: done 1321 episodes, mean reward -200.000, speed 1863.20 f/s\n",
      "266200: done 1331 episodes, mean reward -200.000, speed 1944.67 f/s\n",
      "268200: done 1341 episodes, mean reward -200.000, speed 1875.75 f/s\n",
      "270200: done 1351 episodes, mean reward -200.000, speed 1845.32 f/s\n",
      "272200: done 1361 episodes, mean reward -200.000, speed 1876.14 f/s\n",
      "274400: done 1372 episodes, mean reward -200.000, speed 1967.71 f/s\n",
      "276400: done 1382 episodes, mean reward -200.000, speed 1879.28 f/s\n",
      "278200: done 1391 episodes, mean reward -200.000, speed 1705.55 f/s\n",
      "280000: done 1400 episodes, mean reward -200.000, speed 1799.41 f/s\n",
      "282000: done 1410 episodes, mean reward -200.000, speed 1800.61 f/s\n",
      "284000: done 1420 episodes, mean reward -200.000, speed 1784.58 f/s\n",
      "286000: done 1430 episodes, mean reward -200.000, speed 1940.04 f/s\n",
      "288000: done 1440 episodes, mean reward -200.000, speed 1822.04 f/s\n",
      "289800: done 1449 episodes, mean reward -200.000, speed 1755.10 f/s\n",
      "291800: done 1459 episodes, mean reward -200.000, speed 1810.45 f/s\n",
      "293800: done 1469 episodes, mean reward -200.000, speed 1787.40 f/s\n",
      "295800: done 1479 episodes, mean reward -200.000, speed 1961.43 f/s\n",
      "297800: done 1489 episodes, mean reward -200.000, speed 1800.01 f/s\n",
      "299800: done 1499 episodes, mean reward -200.000, speed 1826.72 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "301600: done 1508 episodes, mean reward -200.000, speed 1649.73 f/s\n",
      "303600: done 1518 episodes, mean reward -200.000, speed 1872.73 f/s\n",
      "305600: done 1528 episodes, mean reward -200.000, speed 1877.20 f/s\n",
      "307600: done 1538 episodes, mean reward -200.000, speed 1956.14 f/s\n",
      "309600: done 1548 episodes, mean reward -200.000, speed 1895.76 f/s\n",
      "311600: done 1558 episodes, mean reward -200.000, speed 1877.74 f/s\n",
      "313600: done 1568 episodes, mean reward -200.000, speed 1891.09 f/s\n",
      "315600: done 1578 episodes, mean reward -200.000, speed 1874.05 f/s\n",
      "317600: done 1588 episodes, mean reward -200.000, speed 1990.32 f/s\n",
      "319600: done 1598 episodes, mean reward -200.000, speed 1868.96 f/s\n",
      "321600: done 1608 episodes, mean reward -200.000, speed 1886.98 f/s\n",
      "323600: done 1618 episodes, mean reward -200.000, speed 1874.98 f/s\n",
      "325800: done 1629 episodes, mean reward -200.000, speed 2002.18 f/s\n",
      "327800: done 1639 episodes, mean reward -200.000, speed 1900.55 f/s\n",
      "329800: done 1649 episodes, mean reward -200.000, speed 1866.26 f/s\n",
      "331800: done 1659 episodes, mean reward -200.000, speed 1846.09 f/s\n",
      "333600: done 1668 episodes, mean reward -200.000, speed 1759.20 f/s\n",
      "335400: done 1677 episodes, mean reward -200.000, speed 1677.38 f/s\n",
      "337200: done 1686 episodes, mean reward -200.000, speed 1726.81 f/s\n",
      "339200: done 1696 episodes, mean reward -200.000, speed 1912.65 f/s\n",
      "341200: done 1706 episodes, mean reward -200.000, speed 1833.10 f/s\n",
      "343200: done 1716 episodes, mean reward -200.000, speed 1820.25 f/s\n",
      "345200: done 1726 episodes, mean reward -200.000, speed 1811.22 f/s\n",
      "347000: done 1735 episodes, mean reward -200.000, speed 1740.13 f/s\n",
      "349000: done 1745 episodes, mean reward -200.000, speed 1970.01 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "350600: done 1753 episodes, mean reward -200.000, speed 1491.28 f/s\n",
      "352600: done 1763 episodes, mean reward -200.000, speed 1941.89 f/s\n",
      "354600: done 1773 episodes, mean reward -200.000, speed 1927.93 f/s\n",
      "356400: done 1782 episodes, mean reward -200.000, speed 1790.29 f/s\n",
      "358400: done 1792 episodes, mean reward -200.000, speed 1870.68 f/s\n",
      "360400: done 1802 episodes, mean reward -200.000, speed 1902.41 f/s\n",
      "362400: done 1812 episodes, mean reward -200.000, speed 1848.06 f/s\n",
      "364400: done 1822 episodes, mean reward -200.000, speed 1930.97 f/s\n",
      "366400: done 1832 episodes, mean reward -200.000, speed 1840.41 f/s\n",
      "368400: done 1842 episodes, mean reward -200.000, speed 1942.25 f/s\n",
      "370400: done 1852 episodes, mean reward -200.000, speed 1876.04 f/s\n",
      "372400: done 1862 episodes, mean reward -200.000, speed 1903.45 f/s\n",
      "374400: done 1872 episodes, mean reward -200.000, speed 1960.34 f/s\n",
      "376400: done 1882 episodes, mean reward -200.000, speed 1834.87 f/s\n",
      "378400: done 1892 episodes, mean reward -200.000, speed 1836.90 f/s\n",
      "380400: done 1902 episodes, mean reward -200.000, speed 1802.99 f/s\n",
      "382200: done 1911 episodes, mean reward -200.000, speed 1750.93 f/s\n",
      "384000: done 1920 episodes, mean reward -200.000, speed 1795.97 f/s\n",
      "386000: done 1930 episodes, mean reward -200.000, speed 1882.45 f/s\n",
      "388000: done 1940 episodes, mean reward -200.000, speed 1846.82 f/s\n",
      "390000: done 1950 episodes, mean reward -200.000, speed 1796.62 f/s\n",
      "391800: done 1959 episodes, mean reward -200.000, speed 1757.40 f/s\n",
      "393800: done 1969 episodes, mean reward -200.000, speed 1823.65 f/s\n",
      "395800: done 1979 episodes, mean reward -200.000, speed 1817.66 f/s\n",
      "397800: done 1989 episodes, mean reward -200.000, speed 1939.61 f/s\n",
      "399800: done 1999 episodes, mean reward -200.000, speed 1845.36 f/s\n",
      "Test done is 0.19 sec, reward -200.00, steps 200\n",
      "401400: done 2007 episodes, mean reward -200.000, speed 1544.31 f/s\n",
      "403400: done 2017 episodes, mean reward -200.000, speed 1870.22 f/s\n",
      "405400: done 2027 episodes, mean reward -200.000, speed 1882.27 f/s\n",
      "407400: done 2037 episodes, mean reward -200.000, speed 1939.22 f/s\n",
      "409400: done 2047 episodes, mean reward -200.000, speed 1987.11 f/s\n",
      "411400: done 2057 episodes, mean reward -200.000, speed 1918.17 f/s\n",
      "413400: done 2067 episodes, mean reward -200.000, speed 1839.21 f/s\n",
      "415400: done 2077 episodes, mean reward -200.000, speed 1827.43 f/s\n",
      "417400: done 2087 episodes, mean reward -200.000, speed 1903.82 f/s\n",
      "419400: done 2097 episodes, mean reward -200.000, speed 1994.28 f/s\n",
      "421400: done 2107 episodes, mean reward -200.000, speed 1918.95 f/s\n",
      "423400: done 2117 episodes, mean reward -200.000, speed 1923.22 f/s\n",
      "425400: done 2127 episodes, mean reward -200.000, speed 1823.20 f/s\n",
      "427200: done 2136 episodes, mean reward -200.000, speed 1700.71 f/s\n",
      "429000: done 2145 episodes, mean reward -200.000, speed 1752.03 f/s\n",
      "431000: done 2155 episodes, mean reward -200.000, speed 1837.63 f/s\n",
      "433000: done 2165 episodes, mean reward -200.000, speed 1939.99 f/s\n",
      "435000: done 2175 episodes, mean reward -200.000, speed 1802.36 f/s\n",
      "437000: done 2185 episodes, mean reward -200.000, speed 1816.01 f/s\n",
      "438800: done 2194 episodes, mean reward -200.000, speed 1780.07 f/s\n",
      "440600: done 2203 episodes, mean reward -200.000, speed 1745.86 f/s\n",
      "442400: done 2212 episodes, mean reward -200.000, speed 1648.10 f/s\n",
      "444400: done 2222 episodes, mean reward -200.000, speed 1843.26 f/s\n",
      "446400: done 2232 episodes, mean reward -200.000, speed 1927.07 f/s\n",
      "448400: done 2242 episodes, mean reward -200.000, speed 1815.98 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "450200: done 2251 episodes, mean reward -200.000, speed 1615.55 f/s\n",
      "452200: done 2261 episodes, mean reward -200.000, speed 1880.85 f/s\n",
      "454200: done 2271 episodes, mean reward -200.000, speed 1876.77 f/s\n",
      "456200: done 2281 episodes, mean reward -200.000, speed 1876.78 f/s\n",
      "458400: done 2292 episodes, mean reward -200.000, speed 1960.81 f/s\n",
      "460400: done 2302 episodes, mean reward -200.000, speed 1903.61 f/s\n",
      "462400: done 2312 episodes, mean reward -200.000, speed 1882.49 f/s\n",
      "464400: done 2322 episodes, mean reward -200.000, speed 1873.78 f/s\n",
      "466400: done 2332 episodes, mean reward -200.000, speed 1990.92 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468400: done 2342 episodes, mean reward -200.000, speed 1873.66 f/s\n",
      "470400: done 2352 episodes, mean reward -200.000, speed 1886.26 f/s\n",
      "472400: done 2362 episodes, mean reward -200.000, speed 1838.57 f/s\n",
      "474200: done 2371 episodes, mean reward -200.000, speed 1750.12 f/s\n",
      "476000: done 2380 episodes, mean reward -200.000, speed 1761.57 f/s\n",
      "478000: done 2390 episodes, mean reward -200.000, speed 1928.92 f/s\n",
      "480000: done 2400 episodes, mean reward -200.000, speed 1807.30 f/s\n",
      "482000: done 2410 episodes, mean reward -200.000, speed 1873.67 f/s\n",
      "484000: done 2420 episodes, mean reward -200.000, speed 1815.17 f/s\n",
      "485800: done 2429 episodes, mean reward -200.000, speed 1774.45 f/s\n",
      "487800: done 2439 episodes, mean reward -200.000, speed 1819.52 f/s\n",
      "489800: done 2449 episodes, mean reward -200.000, speed 1899.52 f/s\n",
      "491800: done 2459 episodes, mean reward -200.000, speed 1816.94 f/s\n",
      "493800: done 2469 episodes, mean reward -200.000, speed 1862.11 f/s\n",
      "495800: done 2479 episodes, mean reward -200.000, speed 1811.61 f/s\n",
      "497800: done 2489 episodes, mean reward -200.000, speed 1982.55 f/s\n",
      "499800: done 2499 episodes, mean reward -200.000, speed 1856.91 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "501600: done 2508 episodes, mean reward -200.000, speed 1664.52 f/s\n",
      "503600: done 2518 episodes, mean reward -200.000, speed 1896.52 f/s\n",
      "505600: done 2528 episodes, mean reward -200.000, speed 1885.79 f/s\n",
      "507600: done 2538 episodes, mean reward -200.000, speed 1842.25 f/s\n",
      "509600: done 2548 episodes, mean reward -200.000, speed 1983.80 f/s\n",
      "511600: done 2558 episodes, mean reward -200.000, speed 1908.21 f/s\n",
      "513600: done 2568 episodes, mean reward -200.000, speed 1858.24 f/s\n",
      "515600: done 2578 episodes, mean reward -200.000, speed 1892.30 f/s\n",
      "517600: done 2588 episodes, mean reward -200.000, speed 1852.00 f/s\n",
      "519600: done 2598 episodes, mean reward -200.000, speed 1948.88 f/s\n",
      "521600: done 2608 episodes, mean reward -200.000, speed 1854.23 f/s\n",
      "523400: done 2617 episodes, mean reward -200.000, speed 1741.57 f/s\n",
      "525400: done 2627 episodes, mean reward -200.000, speed 1804.70 f/s\n",
      "527400: done 2637 episodes, mean reward -200.000, speed 1841.57 f/s\n",
      "529400: done 2647 episodes, mean reward -200.000, speed 1898.02 f/s\n",
      "531400: done 2657 episodes, mean reward -200.000, speed 1800.82 f/s\n",
      "533200: done 2666 episodes, mean reward -200.000, speed 1796.95 f/s\n",
      "535200: done 2676 episodes, mean reward -200.000, speed 1793.37 f/s\n",
      "537200: done 2686 episodes, mean reward -200.000, speed 1842.33 f/s\n",
      "539200: done 2696 episodes, mean reward -200.000, speed 1795.92 f/s\n",
      "541200: done 2706 episodes, mean reward -200.000, speed 1953.81 f/s\n",
      "543000: done 2715 episodes, mean reward -200.000, speed 1776.67 f/s\n",
      "545000: done 2725 episodes, mean reward -200.000, speed 1883.33 f/s\n",
      "547000: done 2735 episodes, mean reward -200.000, speed 1841.76 f/s\n",
      "548800: done 2744 episodes, mean reward -200.000, speed 1788.95 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "550400: done 2752 episodes, mean reward -200.000, speed 1587.65 f/s\n",
      "552400: done 2762 episodes, mean reward -200.000, speed 1890.07 f/s\n",
      "554400: done 2772 episodes, mean reward -200.000, speed 1878.56 f/s\n",
      "556400: done 2782 episodes, mean reward -200.000, speed 1959.29 f/s\n",
      "558400: done 2792 episodes, mean reward -200.000, speed 1915.71 f/s\n",
      "560400: done 2802 episodes, mean reward -200.000, speed 1838.12 f/s\n",
      "562400: done 2812 episodes, mean reward -200.000, speed 1870.81 f/s\n",
      "564200: done 2821 episodes, mean reward -200.000, speed 1797.26 f/s\n",
      "566200: done 2831 episodes, mean reward -200.000, speed 1834.62 f/s\n",
      "568000: done 2840 episodes, mean reward -200.000, speed 1781.54 f/s\n",
      "570000: done 2850 episodes, mean reward -200.000, speed 1939.48 f/s\n",
      "572000: done 2860 episodes, mean reward -200.000, speed 1804.25 f/s\n",
      "574000: done 2870 episodes, mean reward -200.000, speed 1789.84 f/s\n",
      "576000: done 2880 episodes, mean reward -200.000, speed 1839.59 f/s\n",
      "578000: done 2890 episodes, mean reward -200.000, speed 1826.91 f/s\n",
      "579800: done 2899 episodes, mean reward -200.000, speed 1761.62 f/s\n",
      "581800: done 2909 episodes, mean reward -200.000, speed 1937.38 f/s\n",
      "583800: done 2919 episodes, mean reward -200.000, speed 1848.53 f/s\n",
      "585800: done 2929 episodes, mean reward -200.000, speed 1891.97 f/s\n",
      "587800: done 2939 episodes, mean reward -200.000, speed 1902.93 f/s\n",
      "590000: done 2950 episodes, mean reward -200.000, speed 1991.22 f/s\n",
      "592000: done 2960 episodes, mean reward -200.000, speed 1894.47 f/s\n",
      "594000: done 2970 episodes, mean reward -200.000, speed 1854.71 f/s\n",
      "596000: done 2980 episodes, mean reward -200.000, speed 1867.72 f/s\n",
      "598200: done 2991 episodes, mean reward -200.000, speed 1994.40 f/s\n",
      "Test done is 0.14 sec, reward -200.00, steps 200\n",
      "600200: done 3001 episodes, mean reward -200.000, speed 1663.14 f/s\n",
      "602200: done 3011 episodes, mean reward -200.000, speed 1889.12 f/s\n",
      "604200: done 3021 episodes, mean reward -200.000, speed 1886.76 f/s\n",
      "606000: done 3030 episodes, mean reward -200.000, speed 1778.11 f/s\n",
      "608000: done 3040 episodes, mean reward -200.000, speed 1741.71 f/s\n",
      "610000: done 3050 episodes, mean reward -200.000, speed 1802.46 f/s\n",
      "612000: done 3060 episodes, mean reward -200.000, speed 1813.78 f/s\n",
      "614000: done 3070 episodes, mean reward -200.000, speed 1841.09 f/s\n",
      "616000: done 3080 episodes, mean reward -200.000, speed 1952.07 f/s\n",
      "617800: done 3089 episodes, mean reward -200.000, speed 1774.99 f/s\n",
      "619800: done 3099 episodes, mean reward -200.000, speed 1807.24 f/s\n",
      "621800: done 3109 episodes, mean reward -200.000, speed 1813.24 f/s\n",
      "623800: done 3119 episodes, mean reward -200.000, speed 1847.51 f/s\n",
      "625800: done 3129 episodes, mean reward -200.000, speed 1836.65 f/s\n",
      "627800: done 3139 episodes, mean reward -200.000, speed 1969.08 f/s\n",
      "629800: done 3149 episodes, mean reward -200.000, speed 1879.61 f/s\n",
      "631800: done 3159 episodes, mean reward -200.000, speed 1936.42 f/s\n",
      "633800: done 3169 episodes, mean reward -200.000, speed 1873.16 f/s\n",
      "636000: done 3180 episodes, mean reward -200.000, speed 2002.41 f/s\n",
      "638000: done 3190 episodes, mean reward -200.000, speed 1845.23 f/s\n",
      "640000: done 3200 episodes, mean reward -200.000, speed 1907.04 f/s\n",
      "642000: done 3210 episodes, mean reward -200.000, speed 1903.04 f/s\n",
      "644200: done 3221 episodes, mean reward -200.000, speed 1994.67 f/s\n",
      "646200: done 3231 episodes, mean reward -200.000, speed 1907.13 f/s\n",
      "648200: done 3241 episodes, mean reward -200.000, speed 1907.34 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "650200: done 3251 episodes, mean reward -200.000, speed 1674.44 f/s\n",
      "652000: done 3260 episodes, mean reward -200.000, speed 1725.94 f/s\n",
      "653800: done 3269 episodes, mean reward -200.000, speed 1790.57 f/s\n",
      "655800: done 3279 episodes, mean reward -200.000, speed 1809.80 f/s\n",
      "657800: done 3289 episodes, mean reward -200.000, speed 1817.84 f/s\n",
      "659600: done 3298 episodes, mean reward -200.000, speed 1791.68 f/s\n",
      "661600: done 3308 episodes, mean reward -200.000, speed 1826.32 f/s\n",
      "663600: done 3318 episodes, mean reward -200.000, speed 1806.54 f/s\n",
      "665400: done 3327 episodes, mean reward -200.000, speed 1740.26 f/s\n",
      "667600: done 3338 episodes, mean reward -200.000, speed 1959.50 f/s\n",
      "669600: done 3348 episodes, mean reward -200.000, speed 1809.36 f/s\n",
      "671600: done 3358 episodes, mean reward -200.000, speed 1842.58 f/s\n",
      "673600: done 3368 episodes, mean reward -200.000, speed 1847.84 f/s\n",
      "675600: done 3378 episodes, mean reward -200.000, speed 1966.23 f/s\n",
      "677600: done 3388 episodes, mean reward -200.000, speed 1875.17 f/s\n",
      "679600: done 3398 episodes, mean reward -200.000, speed 1877.09 f/s\n",
      "681600: done 3408 episodes, mean reward -200.000, speed 1898.75 f/s\n",
      "683600: done 3418 episodes, mean reward -200.000, speed 1966.90 f/s\n",
      "685600: done 3428 episodes, mean reward -200.000, speed 1884.17 f/s\n",
      "687600: done 3438 episodes, mean reward -200.000, speed 1904.44 f/s\n",
      "689600: done 3448 episodes, mean reward -200.000, speed 1901.64 f/s\n",
      "691600: done 3458 episodes, mean reward -200.000, speed 1898.00 f/s\n",
      "693800: done 3469 episodes, mean reward -200.000, speed 1996.12 f/s\n",
      "695800: done 3479 episodes, mean reward -200.000, speed 1795.93 f/s\n",
      "697800: done 3489 episodes, mean reward -200.000, speed 1804.85 f/s\n",
      "699800: done 3499 episodes, mean reward -200.000, speed 1910.18 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "701400: done 3507 episodes, mean reward -200.000, speed 1432.68 f/s\n",
      "703400: done 3517 episodes, mean reward -200.000, speed 1945.95 f/s\n",
      "705400: done 3527 episodes, mean reward -200.000, speed 1879.66 f/s\n",
      "707400: done 3537 episodes, mean reward -200.000, speed 1873.82 f/s\n",
      "709400: done 3547 episodes, mean reward -200.000, speed 1788.44 f/s\n",
      "711200: done 3556 episodes, mean reward -200.000, speed 1799.91 f/s\n",
      "713200: done 3566 episodes, mean reward -200.000, speed 1825.32 f/s\n",
      "715200: done 3576 episodes, mean reward -200.000, speed 1900.96 f/s\n",
      "717200: done 3586 episodes, mean reward -200.000, speed 1893.27 f/s\n",
      "719200: done 3596 episodes, mean reward -200.000, speed 1876.22 f/s\n",
      "721200: done 3606 episodes, mean reward -200.000, speed 1889.64 f/s\n",
      "723200: done 3616 episodes, mean reward -200.000, speed 1900.10 f/s\n",
      "725200: done 3626 episodes, mean reward -200.000, speed 1961.87 f/s\n",
      "727200: done 3636 episodes, mean reward -200.000, speed 1870.84 f/s\n",
      "729200: done 3646 episodes, mean reward -200.000, speed 1908.99 f/s\n",
      "731200: done 3656 episodes, mean reward -200.000, speed 1914.79 f/s\n",
      "733200: done 3666 episodes, mean reward -200.000, speed 1954.75 f/s\n",
      "735200: done 3676 episodes, mean reward -200.000, speed 1869.87 f/s\n",
      "737200: done 3686 episodes, mean reward -200.000, speed 1849.33 f/s\n",
      "739200: done 3696 episodes, mean reward -200.000, speed 1890.17 f/s\n",
      "741200: done 3706 episodes, mean reward -200.000, speed 1897.39 f/s\n",
      "743400: done 3717 episodes, mean reward -200.000, speed 2007.16 f/s\n",
      "745400: done 3727 episodes, mean reward -200.000, speed 1784.61 f/s\n",
      "747200: done 3736 episodes, mean reward -200.000, speed 1789.35 f/s\n",
      "749000: done 3745 episodes, mean reward -200.000, speed 1780.96 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "750600: done 3753 episodes, mean reward -200.000, speed 1581.92 f/s\n",
      "752600: done 3763 episodes, mean reward -200.000, speed 1819.77 f/s\n",
      "754600: done 3773 episodes, mean reward -200.000, speed 1863.94 f/s\n",
      "756600: done 3783 episodes, mean reward -200.000, speed 1983.52 f/s\n",
      "758400: done 3792 episodes, mean reward -200.000, speed 1774.93 f/s\n",
      "760400: done 3802 episodes, mean reward -200.000, speed 1785.05 f/s\n",
      "762400: done 3812 episodes, mean reward -200.000, speed 1813.76 f/s\n",
      "764400: done 3822 episodes, mean reward -200.000, speed 1831.76 f/s\n",
      "766400: done 3832 episodes, mean reward -200.000, speed 1871.57 f/s\n",
      "768400: done 3842 episodes, mean reward -200.000, speed 1947.16 f/s\n",
      "770400: done 3852 episodes, mean reward -200.000, speed 1877.31 f/s\n",
      "772400: done 3862 episodes, mean reward -200.000, speed 1904.16 f/s\n",
      "774400: done 3872 episodes, mean reward -200.000, speed 1892.68 f/s\n",
      "776400: done 3882 episodes, mean reward -200.000, speed 1885.88 f/s\n",
      "778600: done 3893 episodes, mean reward -200.000, speed 1981.07 f/s\n",
      "780600: done 3903 episodes, mean reward -200.000, speed 1896.24 f/s\n",
      "782600: done 3913 episodes, mean reward -200.000, speed 1870.42 f/s\n",
      "784600: done 3923 episodes, mean reward -200.000, speed 1989.21 f/s\n",
      "786400: done 3932 episodes, mean reward -200.000, speed 1768.86 f/s\n",
      "788400: done 3942 episodes, mean reward -200.000, speed 1921.51 f/s\n",
      "790400: done 3952 episodes, mean reward -200.000, speed 1903.35 f/s\n",
      "792400: done 3962 episodes, mean reward -200.000, speed 1792.93 f/s\n",
      "794200: done 3971 episodes, mean reward -200.000, speed 1760.14 f/s\n",
      "796200: done 3981 episodes, mean reward -200.000, speed 1830.69 f/s\n",
      "798000: done 3990 episodes, mean reward -200.000, speed 1790.59 f/s\n",
      "800000: done 4000 episodes, mean reward -200.000, speed 1910.58 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "801600: done 4008 episodes, mean reward -200.000, speed 1522.13 f/s\n",
      "803600: done 4018 episodes, mean reward -200.000, speed 1956.60 f/s\n",
      "805600: done 4028 episodes, mean reward -200.000, speed 1756.40 f/s\n",
      "807600: done 4038 episodes, mean reward -200.000, speed 1813.85 f/s\n",
      "809600: done 4048 episodes, mean reward -200.000, speed 1818.69 f/s\n",
      "811600: done 4058 episodes, mean reward -200.000, speed 1920.58 f/s\n",
      "813600: done 4068 episodes, mean reward -200.000, speed 1874.38 f/s\n",
      "815600: done 4078 episodes, mean reward -200.000, speed 1834.35 f/s\n",
      "817600: done 4088 episodes, mean reward -200.000, speed 1898.86 f/s\n",
      "819600: done 4098 episodes, mean reward -200.000, speed 1901.53 f/s\n",
      "821600: done 4108 episodes, mean reward -200.000, speed 1951.06 f/s\n",
      "823600: done 4118 episodes, mean reward -200.000, speed 1892.39 f/s\n",
      "825600: done 4128 episodes, mean reward -200.000, speed 1870.96 f/s\n",
      "827600: done 4138 episodes, mean reward -200.000, speed 1877.78 f/s\n",
      "829600: done 4148 episodes, mean reward -200.000, speed 1894.86 f/s\n",
      "831600: done 4158 episodes, mean reward -200.000, speed 1977.93 f/s\n",
      "833400: done 4167 episodes, mean reward -200.000, speed 1744.81 f/s\n",
      "835200: done 4176 episodes, mean reward -200.000, speed 1763.60 f/s\n",
      "837200: done 4186 episodes, mean reward -200.000, speed 1799.81 f/s\n",
      "839200: done 4196 episodes, mean reward -200.000, speed 1861.89 f/s\n",
      "841200: done 4206 episodes, mean reward -200.000, speed 1822.93 f/s\n",
      "843200: done 4216 episodes, mean reward -200.000, speed 1908.66 f/s\n",
      "845000: done 4225 episodes, mean reward -200.000, speed 1730.52 f/s\n",
      "847000: done 4235 episodes, mean reward -200.000, speed 1832.81 f/s\n",
      "849000: done 4245 episodes, mean reward -200.000, speed 1854.89 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "850600: done 4253 episodes, mean reward -200.000, speed 1593.07 f/s\n",
      "852600: done 4263 episodes, mean reward -200.000, speed 1792.51 f/s\n",
      "854600: done 4273 episodes, mean reward -200.000, speed 1806.71 f/s\n",
      "856600: done 4283 episodes, mean reward -200.000, speed 1877.78 f/s\n",
      "858600: done 4293 episodes, mean reward -200.000, speed 1988.24 f/s\n",
      "860600: done 4303 episodes, mean reward -200.000, speed 1832.63 f/s\n",
      "862600: done 4313 episodes, mean reward -200.000, speed 1873.02 f/s\n",
      "864600: done 4323 episodes, mean reward -200.000, speed 1931.25 f/s\n",
      "866600: done 4333 episodes, mean reward -200.000, speed 1986.84 f/s\n",
      "868600: done 4343 episodes, mean reward -200.000, speed 1890.82 f/s\n",
      "870600: done 4353 episodes, mean reward -200.000, speed 1833.34 f/s\n",
      "872600: done 4363 episodes, mean reward -200.000, speed 1791.06 f/s\n",
      "874600: done 4373 episodes, mean reward -200.000, speed 1805.36 f/s\n",
      "876600: done 4383 episodes, mean reward -200.000, speed 1921.42 f/s\n",
      "878600: done 4393 episodes, mean reward -200.000, speed 1884.64 f/s\n",
      "880400: done 4402 episodes, mean reward -200.000, speed 1796.99 f/s\n",
      "882200: done 4411 episodes, mean reward -200.000, speed 1769.07 f/s\n",
      "884200: done 4421 episodes, mean reward -200.000, speed 1852.51 f/s\n",
      "886200: done 4431 episodes, mean reward -200.000, speed 1809.43 f/s\n",
      "888200: done 4441 episodes, mean reward -200.000, speed 1831.64 f/s\n",
      "890200: done 4451 episodes, mean reward -200.000, speed 1926.53 f/s\n",
      "892000: done 4460 episodes, mean reward -200.000, speed 1771.63 f/s\n",
      "894000: done 4470 episodes, mean reward -200.000, speed 1854.26 f/s\n",
      "896000: done 4480 episodes, mean reward -200.000, speed 1924.84 f/s\n",
      "898000: done 4490 episodes, mean reward -200.000, speed 1909.01 f/s\n",
      "900000: done 4500 episodes, mean reward -200.000, speed 1975.37 f/s\n",
      "Test done is 0.15 sec, reward -200.00, steps 200\n",
      "901600: done 4508 episodes, mean reward -200.000, speed 1545.86 f/s\n",
      "903800: done 4519 episodes, mean reward -200.000, speed 1990.12 f/s\n",
      "905800: done 4529 episodes, mean reward -200.000, speed 1911.71 f/s\n",
      "907800: done 4539 episodes, mean reward -200.000, speed 1873.98 f/s\n",
      "909800: done 4549 episodes, mean reward -200.000, speed 1877.01 f/s\n",
      "911800: done 4559 episodes, mean reward -200.000, speed 1990.42 f/s\n",
      "913800: done 4569 episodes, mean reward -200.000, speed 1899.01 f/s\n",
      "915800: done 4579 episodes, mean reward -200.000, speed 1914.43 f/s\n",
      "917600: done 4588 episodes, mean reward -200.000, speed 1756.37 f/s\n",
      "919400: done 4597 episodes, mean reward -200.000, speed 1782.24 f/s\n",
      "921400: done 4607 episodes, mean reward -200.000, speed 1826.42 f/s\n",
      "923400: done 4617 episodes, mean reward -200.000, speed 1889.41 f/s\n",
      "925200: done 4626 episodes, mean reward -200.000, speed 1747.30 f/s\n",
      "927200: done 4636 episodes, mean reward -200.000, speed 1934.98 f/s\n",
      "929000: done 4645 episodes, mean reward -200.000, speed 1769.16 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931000: done 4655 episodes, mean reward -200.000, speed 1822.96 f/s\n",
      "933000: done 4665 episodes, mean reward -200.000, speed 1839.03 f/s\n",
      "935000: done 4675 episodes, mean reward -200.000, speed 1828.42 f/s\n",
      "937000: done 4685 episodes, mean reward -200.000, speed 1934.08 f/s\n",
      "939000: done 4695 episodes, mean reward -200.000, speed 1811.94 f/s\n",
      "941000: done 4705 episodes, mean reward -200.000, speed 1926.54 f/s\n",
      "943000: done 4715 episodes, mean reward -200.000, speed 1886.87 f/s\n",
      "945000: done 4725 episodes, mean reward -200.000, speed 1877.28 f/s\n",
      "947000: done 4735 episodes, mean reward -200.000, speed 1987.39 f/s\n",
      "949000: done 4745 episodes, mean reward -200.000, speed 1912.23 f/s\n",
      "Test done is 0.15 sec, reward -200.00, steps 200\n",
      "950800: done 4754 episodes, mean reward -200.000, speed 1632.01 f/s\n",
      "952800: done 4764 episodes, mean reward -200.000, speed 1906.60 f/s\n",
      "954800: done 4774 episodes, mean reward -200.000, speed 1857.77 f/s\n",
      "956800: done 4784 episodes, mean reward -200.000, speed 1957.92 f/s\n",
      "958800: done 4794 episodes, mean reward -200.000, speed 1782.13 f/s\n",
      "960600: done 4803 episodes, mean reward -200.000, speed 1788.03 f/s\n",
      "962600: done 4813 episodes, mean reward -200.000, speed 1808.65 f/s\n",
      "964400: done 4822 episodes, mean reward -200.000, speed 1799.67 f/s\n",
      "966400: done 4832 episodes, mean reward -200.000, speed 1754.97 f/s\n",
      "968200: done 4841 episodes, mean reward -200.000, speed 1795.77 f/s\n",
      "970200: done 4851 episodes, mean reward -200.000, speed 1835.86 f/s\n",
      "972200: done 4861 episodes, mean reward -200.000, speed 1910.27 f/s\n",
      "974200: done 4871 episodes, mean reward -200.000, speed 1846.65 f/s\n",
      "976000: done 4880 episodes, mean reward -200.000, speed 1788.76 f/s\n",
      "978000: done 4890 episodes, mean reward -200.000, speed 1842.53 f/s\n",
      "980000: done 4900 episodes, mean reward -200.000, speed 1877.14 f/s\n",
      "982000: done 4910 episodes, mean reward -200.000, speed 1851.88 f/s\n",
      "984000: done 4920 episodes, mean reward -200.000, speed 1999.86 f/s\n",
      "986000: done 4930 episodes, mean reward -200.000, speed 1875.76 f/s\n",
      "988000: done 4940 episodes, mean reward -200.000, speed 1872.22 f/s\n",
      "990000: done 4950 episodes, mean reward -200.000, speed 1907.52 f/s\n",
      "992000: done 4960 episodes, mean reward -200.000, speed 1966.18 f/s\n",
      "994000: done 4970 episodes, mean reward -200.000, speed 1852.36 f/s\n",
      "996000: done 4980 episodes, mean reward -200.000, speed 1905.25 f/s\n",
      "998000: done 4990 episodes, mean reward -200.000, speed 1794.16 f/s\n",
      "999800: done 4999 episodes, mean reward -200.000, speed 1788.23 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "1001400: done 5007 episodes, mean reward -200.000, speed 1599.78 f/s\n",
      "1003400: done 5017 episodes, mean reward -200.000, speed 1828.44 f/s\n",
      "1005200: done 5026 episodes, mean reward -200.000, speed 1789.22 f/s\n",
      "1007000: done 5035 episodes, mean reward -200.000, speed 1795.71 f/s\n",
      "1009000: done 5045 episodes, mean reward -200.000, speed 1802.88 f/s\n",
      "1011000: done 5055 episodes, mean reward -200.000, speed 1958.08 f/s\n",
      "1013000: done 5065 episodes, mean reward -200.000, speed 1855.20 f/s\n",
      "1014800: done 5074 episodes, mean reward -200.000, speed 1751.52 f/s\n",
      "1016800: done 5084 episodes, mean reward -200.000, speed 1825.65 f/s\n",
      "1018800: done 5094 episodes, mean reward -200.000, speed 1846.41 f/s\n",
      "1021000: done 5105 episodes, mean reward -200.000, speed 1971.01 f/s\n",
      "1023000: done 5115 episodes, mean reward -200.000, speed 1883.53 f/s\n",
      "1025000: done 5125 episodes, mean reward -200.000, speed 1865.81 f/s\n",
      "1027000: done 5135 episodes, mean reward -200.000, speed 1882.85 f/s\n",
      "1028800: done 5144 episodes, mean reward -200.000, speed 1696.06 f/s\n",
      "1031000: done 5155 episodes, mean reward -200.000, speed 1968.51 f/s\n",
      "1033000: done 5165 episodes, mean reward -200.000, speed 1924.31 f/s\n",
      "1035000: done 5175 episodes, mean reward -200.000, speed 1847.52 f/s\n",
      "1037000: done 5185 episodes, mean reward -200.000, speed 1783.38 f/s\n",
      "1038800: done 5194 episodes, mean reward -200.000, speed 1774.36 f/s\n",
      "1040800: done 5204 episodes, mean reward -200.000, speed 1907.56 f/s\n",
      "1042600: done 5213 episodes, mean reward -200.000, speed 1778.29 f/s\n",
      "1044600: done 5223 episodes, mean reward -200.000, speed 1821.54 f/s\n",
      "1046600: done 5233 episodes, mean reward -200.000, speed 1839.33 f/s\n",
      "1048600: done 5243 episodes, mean reward -200.000, speed 1848.20 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "1050200: done 5251 episodes, mean reward -200.000, speed 1584.29 f/s\n",
      "1052000: done 5260 episodes, mean reward -200.000, speed 1794.94 f/s\n",
      "1054000: done 5270 episodes, mean reward -200.000, speed 1816.46 f/s\n",
      "1056000: done 5280 episodes, mean reward -200.000, speed 1924.03 f/s\n",
      "1058000: done 5290 episodes, mean reward -200.000, speed 1840.65 f/s\n",
      "1060000: done 5300 episodes, mean reward -200.000, speed 1884.20 f/s\n",
      "1062000: done 5310 episodes, mean reward -200.000, speed 1866.97 f/s\n",
      "1064000: done 5320 episodes, mean reward -200.000, speed 1880.64 f/s\n",
      "1066200: done 5331 episodes, mean reward -200.000, speed 1975.58 f/s\n",
      "1068200: done 5341 episodes, mean reward -200.000, speed 1875.41 f/s\n",
      "1070200: done 5351 episodes, mean reward -200.000, speed 1861.69 f/s\n",
      "1072200: done 5361 episodes, mean reward -200.000, speed 1975.36 f/s\n",
      "1074200: done 5371 episodes, mean reward -200.000, speed 1880.83 f/s\n",
      "1076200: done 5381 episodes, mean reward -200.000, speed 1900.33 f/s\n",
      "1078200: done 5391 episodes, mean reward -200.000, speed 1871.41 f/s\n",
      "1080200: done 5401 episodes, mean reward -200.000, speed 1868.02 f/s\n",
      "1082000: done 5410 episodes, mean reward -200.000, speed 1749.71 f/s\n",
      "1084000: done 5420 episodes, mean reward -200.000, speed 1891.26 f/s\n",
      "1086000: done 5430 episodes, mean reward -200.000, speed 1786.74 f/s\n",
      "1088000: done 5440 episodes, mean reward -200.000, speed 1810.24 f/s\n",
      "1089800: done 5449 episodes, mean reward -200.000, speed 1784.79 f/s\n",
      "1091800: done 5459 episodes, mean reward -200.000, speed 1852.13 f/s\n",
      "1093800: done 5469 episodes, mean reward -200.000, speed 1831.54 f/s\n",
      "1095800: done 5479 episodes, mean reward -200.000, speed 1925.29 f/s\n",
      "1097800: done 5489 episodes, mean reward -200.000, speed 1796.08 f/s\n",
      "1099600: done 5498 episodes, mean reward -200.000, speed 1792.14 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1101200: done 5506 episodes, mean reward -200.000, speed 1579.13 f/s\n",
      "1103200: done 5516 episodes, mean reward -200.000, speed 1903.09 f/s\n",
      "1105200: done 5526 episodes, mean reward -200.000, speed 1847.90 f/s\n",
      "1107200: done 5536 episodes, mean reward -200.000, speed 1867.34 f/s\n",
      "1109200: done 5546 episodes, mean reward -200.000, speed 1936.21 f/s\n",
      "1111200: done 5556 episodes, mean reward -200.000, speed 1858.31 f/s\n",
      "1113200: done 5566 episodes, mean reward -200.000, speed 1927.45 f/s\n",
      "1115200: done 5576 episodes, mean reward -200.000, speed 1877.68 f/s\n",
      "1117200: done 5586 episodes, mean reward -200.000, speed 1892.84 f/s\n",
      "1119200: done 5596 episodes, mean reward -200.000, speed 1939.32 f/s\n",
      "1121200: done 5606 episodes, mean reward -200.000, speed 1914.79 f/s\n",
      "1123200: done 5616 episodes, mean reward -200.000, speed 1869.57 f/s\n",
      "1125200: done 5626 episodes, mean reward -200.000, speed 1849.09 f/s\n",
      "1127000: done 5635 episodes, mean reward -200.000, speed 1782.95 f/s\n",
      "1129000: done 5645 episodes, mean reward -200.000, speed 1892.67 f/s\n",
      "1130800: done 5654 episodes, mean reward -200.000, speed 1764.90 f/s\n",
      "1132800: done 5664 episodes, mean reward -200.000, speed 1937.83 f/s\n",
      "1134800: done 5674 episodes, mean reward -200.000, speed 1783.12 f/s\n",
      "1136600: done 5683 episodes, mean reward -200.000, speed 1764.13 f/s\n",
      "1138600: done 5693 episodes, mean reward -200.000, speed 1840.75 f/s\n",
      "1140600: done 5703 episodes, mean reward -200.000, speed 1791.80 f/s\n",
      "1142600: done 5713 episodes, mean reward -200.000, speed 1959.67 f/s\n",
      "1144600: done 5723 episodes, mean reward -200.000, speed 1813.88 f/s\n",
      "1146400: done 5732 episodes, mean reward -200.000, speed 1789.09 f/s\n",
      "1148400: done 5742 episodes, mean reward -200.000, speed 1889.85 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1150200: done 5751 episodes, mean reward -200.000, speed 1642.71 f/s\n",
      "1152200: done 5761 episodes, mean reward -200.000, speed 1898.86 f/s\n",
      "1154200: done 5771 episodes, mean reward -200.000, speed 1891.40 f/s\n",
      "1156400: done 5782 episodes, mean reward -200.000, speed 1975.56 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158400: done 5792 episodes, mean reward -200.000, speed 1905.18 f/s\n",
      "1160400: done 5802 episodes, mean reward -200.000, speed 1910.78 f/s\n",
      "1162400: done 5812 episodes, mean reward -200.000, speed 1960.69 f/s\n",
      "1164400: done 5822 episodes, mean reward -200.000, speed 1858.59 f/s\n",
      "1166400: done 5832 episodes, mean reward -200.000, speed 1805.54 f/s\n",
      "1168400: done 5842 episodes, mean reward -200.000, speed 1804.05 f/s\n",
      "1170400: done 5852 episodes, mean reward -200.000, speed 1790.83 f/s\n",
      "1172400: done 5862 episodes, mean reward -200.000, speed 1915.02 f/s\n",
      "1174200: done 5871 episodes, mean reward -200.000, speed 1779.76 f/s\n",
      "1176200: done 5881 episodes, mean reward -200.000, speed 1796.42 f/s\n",
      "1178200: done 5891 episodes, mean reward -200.000, speed 1844.15 f/s\n",
      "1180200: done 5901 episodes, mean reward -200.000, speed 1818.61 f/s\n",
      "1182200: done 5911 episodes, mean reward -200.000, speed 1785.33 f/s\n",
      "1184000: done 5920 episodes, mean reward -200.000, speed 1760.47 f/s\n",
      "1186000: done 5930 episodes, mean reward -200.000, speed 1951.71 f/s\n",
      "1188000: done 5940 episodes, mean reward -200.000, speed 1863.09 f/s\n",
      "1190000: done 5950 episodes, mean reward -200.000, speed 1886.85 f/s\n",
      "1192000: done 5960 episodes, mean reward -200.000, speed 1896.35 f/s\n",
      "1194000: done 5970 episodes, mean reward -200.000, speed 1941.12 f/s\n",
      "1196000: done 5980 episodes, mean reward -200.000, speed 1938.12 f/s\n",
      "1198000: done 5990 episodes, mean reward -200.000, speed 1907.28 f/s\n",
      "1200000: done 6000 episodes, mean reward -200.000, speed 1893.33 f/s\n",
      "Test done is 0.15 sec, reward -200.00, steps 200\n",
      "1201800: done 6009 episodes, mean reward -200.000, speed 1631.67 f/s\n",
      "1203800: done 6019 episodes, mean reward -200.000, speed 1946.93 f/s\n",
      "1205800: done 6029 episodes, mean reward -200.000, speed 1967.79 f/s\n",
      "1207800: done 6039 episodes, mean reward -200.000, speed 1881.61 f/s\n",
      "1209800: done 6049 episodes, mean reward -200.000, speed 1842.13 f/s\n",
      "1211800: done 6059 episodes, mean reward -200.000, speed 1810.21 f/s\n",
      "1213600: done 6068 episodes, mean reward -200.000, speed 1777.72 f/s\n",
      "1215600: done 6078 episodes, mean reward -200.000, speed 1907.63 f/s\n",
      "1217600: done 6088 episodes, mean reward -200.000, speed 1821.82 f/s\n",
      "1219600: done 6098 episodes, mean reward -200.000, speed 1833.73 f/s\n",
      "1221400: done 6107 episodes, mean reward -200.000, speed 1750.13 f/s\n",
      "1223400: done 6117 episodes, mean reward -200.000, speed 1843.86 f/s\n",
      "1225400: done 6127 episodes, mean reward -200.000, speed 1826.04 f/s\n",
      "1227400: done 6137 episodes, mean reward -200.000, speed 1954.92 f/s\n",
      "1229400: done 6147 episodes, mean reward -200.000, speed 1848.44 f/s\n",
      "1231200: done 6156 episodes, mean reward -200.000, speed 1794.25 f/s\n",
      "1233200: done 6166 episodes, mean reward -200.000, speed 1881.49 f/s\n",
      "1235200: done 6176 episodes, mean reward -200.000, speed 1877.93 f/s\n",
      "1237200: done 6186 episodes, mean reward -200.000, speed 1923.45 f/s\n",
      "1239200: done 6196 episodes, mean reward -200.000, speed 1974.56 f/s\n",
      "1241200: done 6206 episodes, mean reward -200.000, speed 1878.49 f/s\n",
      "1243200: done 6216 episodes, mean reward -200.000, speed 1879.16 f/s\n",
      "1245200: done 6226 episodes, mean reward -200.000, speed 1874.24 f/s\n",
      "1247200: done 6236 episodes, mean reward -200.000, speed 1952.68 f/s\n",
      "1249200: done 6246 episodes, mean reward -200.000, speed 1886.61 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1251000: done 6255 episodes, mean reward -200.000, speed 1664.49 f/s\n",
      "1252800: done 6264 episodes, mean reward -200.000, speed 1782.88 f/s\n",
      "1254800: done 6274 episodes, mean reward -200.000, speed 1785.70 f/s\n",
      "1256800: done 6284 episodes, mean reward -200.000, speed 1831.27 f/s\n",
      "1258600: done 6293 episodes, mean reward -200.000, speed 1762.88 f/s\n",
      "1260400: done 6302 episodes, mean reward -200.000, speed 1696.39 f/s\n",
      "1262200: done 6311 episodes, mean reward -200.000, speed 1768.50 f/s\n",
      "1264200: done 6321 episodes, mean reward -200.000, speed 1829.40 f/s\n",
      "1266200: done 6331 episodes, mean reward -200.000, speed 1913.48 f/s\n",
      "1268200: done 6341 episodes, mean reward -200.000, speed 1785.74 f/s\n",
      "1270000: done 6350 episodes, mean reward -200.000, speed 1783.99 f/s\n",
      "1272000: done 6360 episodes, mean reward -200.000, speed 1856.01 f/s\n",
      "1274000: done 6370 episodes, mean reward -200.000, speed 1847.15 f/s\n",
      "1276200: done 6381 episodes, mean reward -200.000, speed 1971.34 f/s\n",
      "1278200: done 6391 episodes, mean reward -200.000, speed 1872.19 f/s\n",
      "1280200: done 6401 episodes, mean reward -200.000, speed 1894.00 f/s\n",
      "1282200: done 6411 episodes, mean reward -200.000, speed 1882.33 f/s\n",
      "1284200: done 6421 episodes, mean reward -200.000, speed 1981.60 f/s\n",
      "1286200: done 6431 episodes, mean reward -200.000, speed 1884.92 f/s\n",
      "1288200: done 6441 episodes, mean reward -200.000, speed 1859.24 f/s\n",
      "1290200: done 6451 episodes, mean reward -200.000, speed 1921.94 f/s\n",
      "1292200: done 6461 episodes, mean reward -200.000, speed 1885.84 f/s\n",
      "1294000: done 6470 episodes, mean reward -200.000, speed 1758.81 f/s\n",
      "1296000: done 6480 episodes, mean reward -200.000, speed 1972.55 f/s\n",
      "1297800: done 6489 episodes, mean reward -200.000, speed 1794.87 f/s\n",
      "1299800: done 6499 episodes, mean reward -200.000, speed 1804.82 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "1301400: done 6507 episodes, mean reward -200.000, speed 1592.75 f/s\n",
      "1303400: done 6517 episodes, mean reward -200.000, speed 1838.78 f/s\n",
      "1305400: done 6527 episodes, mean reward -200.000, speed 1828.08 f/s\n",
      "1307200: done 6536 episodes, mean reward -200.000, speed 1757.57 f/s\n",
      "1309200: done 6546 episodes, mean reward -200.000, speed 1830.65 f/s\n",
      "1311200: done 6556 episodes, mean reward -200.000, speed 1915.79 f/s\n",
      "1313200: done 6566 episodes, mean reward -200.000, speed 1837.95 f/s\n",
      "1315200: done 6576 episodes, mean reward -200.000, speed 1828.22 f/s\n",
      "1317200: done 6586 episodes, mean reward -200.000, speed 1917.80 f/s\n",
      "1319200: done 6596 episodes, mean reward -200.000, speed 1877.25 f/s\n",
      "1321200: done 6606 episodes, mean reward -200.000, speed 1982.62 f/s\n",
      "1323200: done 6616 episodes, mean reward -200.000, speed 1836.70 f/s\n",
      "1325200: done 6626 episodes, mean reward -200.000, speed 1873.54 f/s\n",
      "1327200: done 6636 episodes, mean reward -200.000, speed 1876.85 f/s\n",
      "1329200: done 6646 episodes, mean reward -200.000, speed 1986.66 f/s\n",
      "1331200: done 6656 episodes, mean reward -200.000, speed 1868.86 f/s\n",
      "1333200: done 6666 episodes, mean reward -200.000, speed 1861.12 f/s\n",
      "1335200: done 6676 episodes, mean reward -200.000, speed 1903.64 f/s\n",
      "1337200: done 6686 episodes, mean reward -200.000, speed 1840.15 f/s\n",
      "1339000: done 6695 episodes, mean reward -200.000, speed 1763.57 f/s\n",
      "1341000: done 6705 episodes, mean reward -200.000, speed 1887.99 f/s\n",
      "1343000: done 6715 episodes, mean reward -200.000, speed 1822.94 f/s\n",
      "1345000: done 6725 episodes, mean reward -200.000, speed 1790.79 f/s\n",
      "1347000: done 6735 episodes, mean reward -200.000, speed 1819.01 f/s\n",
      "1349000: done 6745 episodes, mean reward -200.000, speed 1813.93 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1350600: done 6753 episodes, mean reward -200.000, speed 1585.61 f/s\n",
      "1352600: done 6763 episodes, mean reward -200.000, speed 1834.80 f/s\n",
      "1354400: done 6772 episodes, mean reward -200.000, speed 1793.49 f/s\n",
      "1356400: done 6782 episodes, mean reward -200.000, speed 1934.13 f/s\n",
      "1358400: done 6792 episodes, mean reward -200.000, speed 1842.01 f/s\n",
      "1360400: done 6802 episodes, mean reward -200.000, speed 1874.14 f/s\n",
      "1362400: done 6812 episodes, mean reward -200.000, speed 1858.79 f/s\n",
      "1364600: done 6823 episodes, mean reward -200.000, speed 2001.00 f/s\n",
      "1366600: done 6833 episodes, mean reward -200.000, speed 1889.61 f/s\n",
      "1368600: done 6843 episodes, mean reward -200.000, speed 1858.43 f/s\n",
      "1370600: done 6853 episodes, mean reward -200.000, speed 1894.18 f/s\n",
      "1372800: done 6864 episodes, mean reward -200.000, speed 1954.54 f/s\n",
      "1374800: done 6874 episodes, mean reward -200.000, speed 1872.87 f/s\n",
      "1376800: done 6884 episodes, mean reward -200.000, speed 1863.48 f/s\n",
      "1378800: done 6894 episodes, mean reward -200.000, speed 1926.00 f/s\n",
      "1380600: done 6903 episodes, mean reward -200.000, speed 1775.18 f/s\n",
      "1382600: done 6913 episodes, mean reward -200.000, speed 1811.99 f/s\n",
      "1384600: done 6923 episodes, mean reward -200.000, speed 1764.54 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386600: done 6933 episodes, mean reward -200.000, speed 1853.88 f/s\n",
      "1388600: done 6943 episodes, mean reward -200.000, speed 1805.26 f/s\n",
      "1390600: done 6953 episodes, mean reward -200.000, speed 1928.46 f/s\n",
      "1392400: done 6962 episodes, mean reward -200.000, speed 1789.69 f/s\n",
      "1394400: done 6972 episodes, mean reward -200.000, speed 1837.85 f/s\n",
      "1396400: done 6982 episodes, mean reward -200.000, speed 1835.91 f/s\n",
      "1398400: done 6992 episodes, mean reward -200.000, speed 1847.82 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1400200: done 7001 episodes, mean reward -200.000, speed 1608.06 f/s\n",
      "1402200: done 7011 episodes, mean reward -200.000, speed 1854.95 f/s\n",
      "1404200: done 7021 episodes, mean reward -200.000, speed 1963.41 f/s\n",
      "1406200: done 7031 episodes, mean reward -200.000, speed 1886.18 f/s\n",
      "1408200: done 7041 episodes, mean reward -200.000, speed 1924.78 f/s\n",
      "1410200: done 7051 episodes, mean reward -200.000, speed 1900.66 f/s\n",
      "1412200: done 7061 episodes, mean reward -200.000, speed 1965.17 f/s\n",
      "1414200: done 7071 episodes, mean reward -200.000, speed 1899.33 f/s\n",
      "1416200: done 7081 episodes, mean reward -200.000, speed 1855.82 f/s\n",
      "1418200: done 7091 episodes, mean reward -200.000, speed 1907.57 f/s\n",
      "1420200: done 7101 episodes, mean reward -200.000, speed 1859.23 f/s\n",
      "1422000: done 7110 episodes, mean reward -200.000, speed 1770.45 f/s\n",
      "1424000: done 7120 episodes, mean reward -200.000, speed 1905.80 f/s\n",
      "1426000: done 7130 episodes, mean reward -200.000, speed 1835.60 f/s\n",
      "1428000: done 7140 episodes, mean reward -200.000, speed 1816.75 f/s\n",
      "1430000: done 7150 episodes, mean reward -200.000, speed 1763.85 f/s\n",
      "1432000: done 7160 episodes, mean reward -200.000, speed 1915.41 f/s\n",
      "1434000: done 7170 episodes, mean reward -200.000, speed 1829.08 f/s\n",
      "1436000: done 7180 episodes, mean reward -200.000, speed 1799.35 f/s\n",
      "1438000: done 7190 episodes, mean reward -200.000, speed 1870.34 f/s\n",
      "1439800: done 7199 episodes, mean reward -200.000, speed 1767.15 f/s\n",
      "1441800: done 7209 episodes, mean reward -200.000, speed 1814.44 f/s\n",
      "1443800: done 7219 episodes, mean reward -200.000, speed 1988.16 f/s\n",
      "1445800: done 7229 episodes, mean reward -200.000, speed 1921.87 f/s\n",
      "1447800: done 7239 episodes, mean reward -200.000, speed 1904.23 f/s\n",
      "1449800: done 7249 episodes, mean reward -200.000, speed 1876.49 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1451600: done 7258 episodes, mean reward -200.000, speed 1664.88 f/s\n",
      "1453600: done 7268 episodes, mean reward -200.000, speed 1899.63 f/s\n",
      "1455600: done 7278 episodes, mean reward -200.000, speed 1995.21 f/s\n",
      "1457600: done 7288 episodes, mean reward -200.000, speed 1850.46 f/s\n",
      "1459600: done 7298 episodes, mean reward -200.000, speed 1861.61 f/s\n",
      "1461600: done 7308 episodes, mean reward -200.000, speed 1890.53 f/s\n",
      "1463600: done 7318 episodes, mean reward -200.000, speed 1912.37 f/s\n",
      "1465400: done 7327 episodes, mean reward -200.000, speed 1798.47 f/s\n",
      "1467400: done 7337 episodes, mean reward -200.000, speed 1818.18 f/s\n",
      "1469200: done 7346 episodes, mean reward -200.000, speed 1779.97 f/s\n",
      "1471200: done 7356 episodes, mean reward -200.000, speed 1807.53 f/s\n",
      "1473200: done 7366 episodes, mean reward -200.000, speed 1796.12 f/s\n",
      "1475200: done 7376 episodes, mean reward -200.000, speed 1865.53 f/s\n",
      "1477200: done 7386 episodes, mean reward -200.000, speed 1896.28 f/s\n",
      "1479200: done 7396 episodes, mean reward -200.000, speed 1818.57 f/s\n",
      "1481200: done 7406 episodes, mean reward -200.000, speed 1884.58 f/s\n",
      "1483200: done 7416 episodes, mean reward -200.000, speed 1819.97 f/s\n",
      "1485200: done 7426 episodes, mean reward -200.000, speed 1943.68 f/s\n",
      "1487200: done 7436 episodes, mean reward -200.000, speed 1824.50 f/s\n",
      "1489200: done 7446 episodes, mean reward -200.000, speed 1875.64 f/s\n",
      "1491200: done 7456 episodes, mean reward -200.000, speed 1877.45 f/s\n",
      "1493200: done 7466 episodes, mean reward -200.000, speed 1877.86 f/s\n",
      "1495200: done 7476 episodes, mean reward -200.000, speed 1966.30 f/s\n",
      "1497200: done 7486 episodes, mean reward -200.000, speed 1893.45 f/s\n",
      "1499200: done 7496 episodes, mean reward -200.000, speed 1885.85 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1501000: done 7505 episodes, mean reward -200.000, speed 1644.28 f/s\n",
      "1503000: done 7515 episodes, mean reward -200.000, speed 1873.74 f/s\n",
      "1505000: done 7525 episodes, mean reward -200.000, speed 1879.88 f/s\n",
      "1507000: done 7535 episodes, mean reward -200.000, speed 1906.34 f/s\n",
      "1508800: done 7544 episodes, mean reward -200.000, speed 1767.13 f/s\n",
      "1510800: done 7554 episodes, mean reward -200.000, speed 1808.62 f/s\n",
      "1512800: done 7564 episodes, mean reward -200.000, speed 1854.88 f/s\n",
      "1514800: done 7574 episodes, mean reward -200.000, speed 1847.10 f/s\n",
      "1516600: done 7583 episodes, mean reward -200.000, speed 1741.76 f/s\n",
      "1518600: done 7593 episodes, mean reward -200.000, speed 1922.91 f/s\n",
      "1520600: done 7603 episodes, mean reward -200.000, speed 1882.89 f/s\n",
      "1522600: done 7613 episodes, mean reward -200.000, speed 1820.68 f/s\n",
      "1524600: done 7623 episodes, mean reward -200.000, speed 1785.29 f/s\n",
      "1526600: done 7633 episodes, mean reward -200.000, speed 1820.63 f/s\n",
      "1528600: done 7643 episodes, mean reward -200.000, speed 1922.90 f/s\n",
      "1530600: done 7653 episodes, mean reward -200.000, speed 1883.62 f/s\n",
      "1532600: done 7663 episodes, mean reward -200.000, speed 1847.58 f/s\n",
      "1534600: done 7673 episodes, mean reward -200.000, speed 1894.87 f/s\n",
      "1536600: done 7683 episodes, mean reward -200.000, speed 1969.04 f/s\n",
      "1538600: done 7693 episodes, mean reward -200.000, speed 1913.11 f/s\n",
      "1540600: done 7703 episodes, mean reward -200.000, speed 1936.79 f/s\n",
      "1542600: done 7713 episodes, mean reward -200.000, speed 1877.45 f/s\n",
      "1544600: done 7723 episodes, mean reward -200.000, speed 1894.13 f/s\n",
      "1546800: done 7734 episodes, mean reward -200.000, speed 1959.51 f/s\n",
      "1548800: done 7744 episodes, mean reward -200.000, speed 1852.37 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "1550400: done 7752 episodes, mean reward -200.000, speed 1517.25 f/s\n",
      "1552400: done 7762 episodes, mean reward -200.000, speed 1858.51 f/s\n",
      "1554200: done 7771 episodes, mean reward -200.000, speed 1772.94 f/s\n",
      "1556200: done 7781 episodes, mean reward -200.000, speed 1801.85 f/s\n",
      "1558200: done 7791 episodes, mean reward -200.000, speed 1872.10 f/s\n",
      "1560200: done 7801 episodes, mean reward -200.000, speed 1987.22 f/s\n",
      "1562200: done 7811 episodes, mean reward -200.000, speed 1793.02 f/s\n",
      "1564000: done 7820 episodes, mean reward -200.000, speed 1799.88 f/s\n",
      "1566000: done 7830 episodes, mean reward -200.000, speed 1800.29 f/s\n",
      "1568000: done 7840 episodes, mean reward -200.000, speed 1874.34 f/s\n",
      "1570000: done 7850 episodes, mean reward -200.000, speed 1955.53 f/s\n",
      "1572000: done 7860 episodes, mean reward -200.000, speed 1846.26 f/s\n",
      "1574000: done 7870 episodes, mean reward -200.000, speed 1881.61 f/s\n",
      "1576000: done 7880 episodes, mean reward -200.000, speed 1905.48 f/s\n",
      "1578000: done 7890 episodes, mean reward -200.000, speed 1902.78 f/s\n",
      "1580000: done 7900 episodes, mean reward -200.000, speed 1997.16 f/s\n",
      "1582000: done 7910 episodes, mean reward -200.000, speed 1865.04 f/s\n",
      "1584000: done 7920 episodes, mean reward -200.000, speed 1886.95 f/s\n",
      "1586000: done 7930 episodes, mean reward -200.000, speed 1866.30 f/s\n",
      "1587800: done 7939 episodes, mean reward -200.000, speed 1759.17 f/s\n",
      "1589800: done 7949 episodes, mean reward -200.000, speed 1889.47 f/s\n",
      "1591800: done 7959 episodes, mean reward -200.000, speed 1777.85 f/s\n",
      "1593800: done 7969 episodes, mean reward -200.000, speed 1844.49 f/s\n",
      "1595800: done 7979 episodes, mean reward -200.000, speed 1840.07 f/s\n",
      "1597800: done 7989 episodes, mean reward -200.000, speed 1877.52 f/s\n",
      "1599800: done 7999 episodes, mean reward -200.000, speed 1912.58 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1601400: done 8007 episodes, mean reward -200.000, speed 1475.67 f/s\n",
      "1603400: done 8017 episodes, mean reward -200.000, speed 1929.74 f/s\n",
      "1605400: done 8027 episodes, mean reward -200.000, speed 1846.45 f/s\n",
      "1607400: done 8037 episodes, mean reward -200.000, speed 1836.51 f/s\n",
      "1609400: done 8047 episodes, mean reward -200.000, speed 1928.65 f/s\n",
      "1611400: done 8057 episodes, mean reward -200.000, speed 1889.19 f/s\n",
      "1613400: done 8067 episodes, mean reward -200.000, speed 1993.90 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615400: done 8077 episodes, mean reward -200.000, speed 1827.87 f/s\n",
      "1617400: done 8087 episodes, mean reward -200.000, speed 1892.90 f/s\n",
      "1619400: done 8097 episodes, mean reward -200.000, speed 1907.64 f/s\n",
      "1621400: done 8107 episodes, mean reward -200.000, speed 1993.36 f/s\n",
      "1623400: done 8117 episodes, mean reward -200.000, speed 1856.39 f/s\n",
      "1625400: done 8127 episodes, mean reward -200.000, speed 1854.48 f/s\n",
      "1627400: done 8137 episodes, mean reward -200.000, speed 1875.49 f/s\n",
      "1629400: done 8147 episodes, mean reward -200.000, speed 1833.86 f/s\n",
      "1631200: done 8156 episodes, mean reward -200.000, speed 1677.58 f/s\n",
      "1633200: done 8166 episodes, mean reward -200.000, speed 1917.99 f/s\n",
      "1635200: done 8176 episodes, mean reward -200.000, speed 1825.92 f/s\n",
      "1637200: done 8186 episodes, mean reward -200.000, speed 1833.74 f/s\n",
      "1639000: done 8195 episodes, mean reward -200.000, speed 1799.88 f/s\n",
      "1641000: done 8205 episodes, mean reward -200.000, speed 1799.92 f/s\n",
      "1643000: done 8215 episodes, mean reward -200.000, speed 1901.79 f/s\n",
      "1644800: done 8224 episodes, mean reward -200.000, speed 1738.56 f/s\n",
      "1646800: done 8234 episodes, mean reward -200.000, speed 1895.57 f/s\n",
      "1648600: done 8243 episodes, mean reward -200.000, speed 1779.58 f/s\n",
      "Test done is 0.17 sec, reward -200.00, steps 200\n",
      "1650200: done 8251 episodes, mean reward -200.000, speed 1460.49 f/s\n",
      "1652400: done 8262 episodes, mean reward -200.000, speed 1961.70 f/s\n",
      "1654400: done 8272 episodes, mean reward -200.000, speed 1828.21 f/s\n",
      "1656400: done 8282 episodes, mean reward -200.000, speed 1803.98 f/s\n",
      "1658200: done 8291 episodes, mean reward -200.000, speed 1785.59 f/s\n",
      "1660200: done 8301 episodes, mean reward -200.000, speed 1926.80 f/s\n",
      "1662200: done 8311 episodes, mean reward -200.000, speed 1796.15 f/s\n",
      "1664000: done 8320 episodes, mean reward -200.000, speed 1712.35 f/s\n",
      "1665800: done 8329 episodes, mean reward -200.000, speed 1778.21 f/s\n",
      "1667800: done 8339 episodes, mean reward -200.000, speed 1819.43 f/s\n",
      "1669800: done 8349 episodes, mean reward -200.000, speed 1805.99 f/s\n",
      "1671600: done 8358 episodes, mean reward -200.000, speed 1769.00 f/s\n",
      "1673600: done 8368 episodes, mean reward -200.000, speed 1812.98 f/s\n",
      "1675600: done 8378 episodes, mean reward -200.000, speed 1944.48 f/s\n",
      "1677600: done 8388 episodes, mean reward -200.000, speed 1795.94 f/s\n",
      "1679600: done 8398 episodes, mean reward -200.000, speed 1828.41 f/s\n",
      "1681600: done 8408 episodes, mean reward -200.000, speed 1872.81 f/s\n",
      "1683600: done 8418 episodes, mean reward -200.000, speed 1983.66 f/s\n",
      "1685600: done 8428 episodes, mean reward -200.000, speed 1851.24 f/s\n",
      "1687600: done 8438 episodes, mean reward -200.000, speed 1804.35 f/s\n",
      "1689600: done 8448 episodes, mean reward -200.000, speed 1835.79 f/s\n",
      "1691600: done 8458 episodes, mean reward -200.000, speed 1867.30 f/s\n",
      "1693600: done 8468 episodes, mean reward -200.000, speed 1961.83 f/s\n",
      "1695600: done 8478 episodes, mean reward -200.000, speed 1936.55 f/s\n",
      "1697600: done 8488 episodes, mean reward -200.000, speed 1913.99 f/s\n",
      "1699600: done 8498 episodes, mean reward -200.000, speed 1897.31 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1701400: done 8507 episodes, mean reward -200.000, speed 1648.88 f/s\n",
      "1703400: done 8517 episodes, mean reward -200.000, speed 1887.40 f/s\n",
      "1705600: done 8528 episodes, mean reward -200.000, speed 1998.45 f/s\n",
      "1707600: done 8538 episodes, mean reward -200.000, speed 1919.33 f/s\n",
      "1709600: done 8548 episodes, mean reward -200.000, speed 1879.41 f/s\n",
      "1711600: done 8558 episodes, mean reward -200.000, speed 1995.90 f/s\n",
      "1713400: done 8567 episodes, mean reward -200.000, speed 1787.18 f/s\n",
      "1715400: done 8577 episodes, mean reward -200.000, speed 1793.57 f/s\n",
      "1717400: done 8587 episodes, mean reward -200.000, speed 1820.52 f/s\n",
      "1719400: done 8597 episodes, mean reward -200.000, speed 1822.68 f/s\n",
      "1721400: done 8607 episodes, mean reward -200.000, speed 1856.07 f/s\n",
      "1723400: done 8617 episodes, mean reward -200.000, speed 1941.76 f/s\n",
      "1725400: done 8627 episodes, mean reward -200.000, speed 1859.91 f/s\n",
      "1727400: done 8637 episodes, mean reward -200.000, speed 1847.22 f/s\n",
      "1729400: done 8647 episodes, mean reward -200.000, speed 1843.54 f/s\n",
      "1731400: done 8657 episodes, mean reward -200.000, speed 1941.59 f/s\n",
      "1733400: done 8667 episodes, mean reward -200.000, speed 1817.65 f/s\n",
      "1735400: done 8677 episodes, mean reward -200.000, speed 1886.81 f/s\n",
      "1737400: done 8687 episodes, mean reward -200.000, speed 1900.99 f/s\n",
      "1739400: done 8697 episodes, mean reward -200.000, speed 1886.46 f/s\n",
      "1741600: done 8708 episodes, mean reward -200.000, speed 1971.96 f/s\n",
      "1743600: done 8718 episodes, mean reward -200.000, speed 1913.00 f/s\n",
      "1745600: done 8728 episodes, mean reward -200.000, speed 1877.73 f/s\n",
      "1747600: done 8738 episodes, mean reward -200.000, speed 1887.43 f/s\n",
      "1749800: done 8749 episodes, mean reward -200.000, speed 1972.87 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1751600: done 8758 episodes, mean reward -200.000, speed 1606.63 f/s\n",
      "1753400: done 8767 episodes, mean reward -200.000, speed 1758.92 f/s\n",
      "1755400: done 8777 episodes, mean reward -200.000, speed 1781.41 f/s\n",
      "1757400: done 8787 episodes, mean reward -200.000, speed 1809.68 f/s\n",
      "1759400: done 8797 episodes, mean reward -200.000, speed 1910.20 f/s\n",
      "1761400: done 8807 episodes, mean reward -200.000, speed 1840.44 f/s\n",
      "1763400: done 8817 episodes, mean reward -200.000, speed 1846.09 f/s\n",
      "1765200: done 8826 episodes, mean reward -200.000, speed 1791.66 f/s\n",
      "1767200: done 8836 episodes, mean reward -200.000, speed 1852.21 f/s\n",
      "1769200: done 8846 episodes, mean reward -200.000, speed 1822.37 f/s\n",
      "1771200: done 8856 episodes, mean reward -200.000, speed 1907.55 f/s\n",
      "1773200: done 8866 episodes, mean reward -200.000, speed 1799.86 f/s\n",
      "1775200: done 8876 episodes, mean reward -200.000, speed 1882.27 f/s\n",
      "1777200: done 8886 episodes, mean reward -200.000, speed 1910.31 f/s\n",
      "1779200: done 8896 episodes, mean reward -200.000, speed 1969.16 f/s\n",
      "1781200: done 8906 episodes, mean reward -200.000, speed 1905.63 f/s\n",
      "1783200: done 8916 episodes, mean reward -200.000, speed 1890.26 f/s\n",
      "1785200: done 8926 episodes, mean reward -200.000, speed 1859.31 f/s\n",
      "1787200: done 8936 episodes, mean reward -200.000, speed 1903.29 f/s\n",
      "1789200: done 8946 episodes, mean reward -200.000, speed 1996.03 f/s\n",
      "1791200: done 8956 episodes, mean reward -200.000, speed 1836.08 f/s\n",
      "1793000: done 8965 episodes, mean reward -200.000, speed 1720.10 f/s\n",
      "1795000: done 8975 episodes, mean reward -200.000, speed 1847.28 f/s\n",
      "1797000: done 8985 episodes, mean reward -200.000, speed 1857.29 f/s\n",
      "1799000: done 8995 episodes, mean reward -200.000, speed 1857.80 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1800600: done 9003 episodes, mean reward -200.000, speed 1586.19 f/s\n",
      "1802600: done 9013 episodes, mean reward -200.000, speed 1857.21 f/s\n",
      "1804600: done 9023 episodes, mean reward -200.000, speed 1934.21 f/s\n",
      "1806600: done 9033 episodes, mean reward -200.000, speed 1873.94 f/s\n",
      "1808600: done 9043 episodes, mean reward -200.000, speed 1854.68 f/s\n",
      "1810600: done 9053 episodes, mean reward -200.000, speed 1823.22 f/s\n",
      "1812600: done 9063 episodes, mean reward -200.000, speed 1915.80 f/s\n",
      "1814600: done 9073 episodes, mean reward -200.000, speed 1863.48 f/s\n",
      "1816600: done 9083 episodes, mean reward -200.000, speed 1893.22 f/s\n",
      "1818600: done 9093 episodes, mean reward -200.000, speed 1882.19 f/s\n",
      "1820600: done 9103 episodes, mean reward -200.000, speed 1948.13 f/s\n",
      "1822800: done 9114 episodes, mean reward -200.000, speed 2008.80 f/s\n",
      "1824800: done 9124 episodes, mean reward -200.000, speed 1918.22 f/s\n",
      "1826800: done 9134 episodes, mean reward -200.000, speed 1925.65 f/s\n",
      "1828800: done 9144 episodes, mean reward -200.000, speed 1972.07 f/s\n",
      "1830800: done 9154 episodes, mean reward -200.000, speed 1817.30 f/s\n",
      "1832800: done 9164 episodes, mean reward -200.000, speed 1883.33 f/s\n",
      "1834800: done 9174 episodes, mean reward -200.000, speed 1898.58 f/s\n",
      "1836800: done 9184 episodes, mean reward -200.000, speed 1929.70 f/s\n",
      "1838800: done 9194 episodes, mean reward -200.000, speed 1966.11 f/s\n",
      "1840800: done 9204 episodes, mean reward -200.000, speed 1895.74 f/s\n",
      "1842800: done 9214 episodes, mean reward -200.000, speed 1919.89 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1844800: done 9224 episodes, mean reward -200.000, speed 1876.72 f/s\n",
      "1847000: done 9235 episodes, mean reward -200.000, speed 1983.81 f/s\n",
      "1849000: done 9245 episodes, mean reward -200.000, speed 1870.13 f/s\n",
      "Test done is 0.14 sec, reward -200.00, steps 200\n",
      "1850600: done 9253 episodes, mean reward -200.000, speed 1590.36 f/s\n",
      "1852600: done 9263 episodes, mean reward -200.000, speed 1887.44 f/s\n",
      "1854600: done 9273 episodes, mean reward -200.000, speed 1898.68 f/s\n",
      "1856600: done 9283 episodes, mean reward -200.000, speed 1868.24 f/s\n",
      "1858800: done 9294 episodes, mean reward -200.000, speed 2007.48 f/s\n",
      "1860800: done 9304 episodes, mean reward -200.000, speed 1913.14 f/s\n",
      "1862800: done 9314 episodes, mean reward -200.000, speed 1873.46 f/s\n",
      "1864800: done 9324 episodes, mean reward -200.000, speed 1865.04 f/s\n",
      "1866800: done 9334 episodes, mean reward -200.000, speed 1987.24 f/s\n",
      "1868800: done 9344 episodes, mean reward -200.000, speed 1914.56 f/s\n",
      "1870800: done 9354 episodes, mean reward -200.000, speed 1907.00 f/s\n",
      "1872800: done 9364 episodes, mean reward -200.000, speed 1931.32 f/s\n",
      "1875000: done 9375 episodes, mean reward -200.000, speed 1956.64 f/s\n",
      "1877000: done 9385 episodes, mean reward -200.000, speed 1902.59 f/s\n",
      "1879000: done 9395 episodes, mean reward -200.000, speed 1916.58 f/s\n",
      "1881000: done 9405 episodes, mean reward -200.000, speed 1894.62 f/s\n",
      "1883200: done 9416 episodes, mean reward -200.000, speed 1968.16 f/s\n",
      "1885200: done 9426 episodes, mean reward -200.000, speed 1961.41 f/s\n",
      "1887200: done 9436 episodes, mean reward -200.000, speed 1885.92 f/s\n",
      "1889400: done 9447 episodes, mean reward -200.000, speed 1984.90 f/s\n",
      "1891400: done 9457 episodes, mean reward -200.000, speed 1863.41 f/s\n",
      "1893400: done 9467 episodes, mean reward -200.000, speed 1897.48 f/s\n",
      "1895400: done 9477 episodes, mean reward -200.000, speed 1942.43 f/s\n",
      "1897600: done 9488 episodes, mean reward -200.000, speed 1992.35 f/s\n",
      "1899600: done 9498 episodes, mean reward -200.000, speed 1876.52 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1901400: done 9507 episodes, mean reward -200.000, speed 1666.19 f/s\n",
      "1903400: done 9517 episodes, mean reward -200.000, speed 1871.64 f/s\n",
      "1905400: done 9527 episodes, mean reward -200.000, speed 1873.97 f/s\n",
      "1907400: done 9537 episodes, mean reward -200.000, speed 1974.39 f/s\n",
      "1909400: done 9547 episodes, mean reward -200.000, speed 1887.84 f/s\n",
      "1911400: done 9557 episodes, mean reward -200.000, speed 1879.97 f/s\n",
      "1913400: done 9567 episodes, mean reward -200.000, speed 1928.27 f/s\n",
      "1915600: done 9578 episodes, mean reward -200.000, speed 1959.64 f/s\n",
      "1917600: done 9588 episodes, mean reward -200.000, speed 1897.68 f/s\n",
      "1919600: done 9598 episodes, mean reward -200.000, speed 1914.08 f/s\n",
      "1921600: done 9608 episodes, mean reward -200.000, speed 1910.34 f/s\n",
      "1923600: done 9618 episodes, mean reward -200.000, speed 1998.18 f/s\n",
      "1925600: done 9628 episodes, mean reward -200.000, speed 1876.63 f/s\n",
      "1927600: done 9638 episodes, mean reward -200.000, speed 1930.89 f/s\n",
      "1929600: done 9648 episodes, mean reward -200.000, speed 1907.55 f/s\n",
      "1931600: done 9658 episodes, mean reward -200.000, speed 1989.93 f/s\n",
      "1933600: done 9668 episodes, mean reward -200.000, speed 1924.62 f/s\n",
      "1935600: done 9678 episodes, mean reward -200.000, speed 1875.97 f/s\n",
      "1937600: done 9688 episodes, mean reward -200.000, speed 1912.41 f/s\n",
      "1939600: done 9698 episodes, mean reward -200.000, speed 1828.38 f/s\n",
      "1941800: done 9709 episodes, mean reward -200.000, speed 1968.32 f/s\n",
      "1943800: done 9719 episodes, mean reward -200.000, speed 1958.21 f/s\n",
      "1945800: done 9729 episodes, mean reward -200.000, speed 1900.88 f/s\n",
      "1947800: done 9739 episodes, mean reward -200.000, speed 1973.89 f/s\n",
      "1949800: done 9749 episodes, mean reward -200.000, speed 1905.28 f/s\n",
      "Test done is 0.16 sec, reward -200.00, steps 200\n",
      "1951600: done 9758 episodes, mean reward -200.000, speed 1665.45 f/s\n",
      "1953600: done 9768 episodes, mean reward -200.000, speed 1911.40 f/s\n",
      "1955600: done 9778 episodes, mean reward -200.000, speed 1880.27 f/s\n",
      "1957600: done 9788 episodes, mean reward -200.000, speed 1868.31 f/s\n",
      "1959800: done 9799 episodes, mean reward -200.000, speed 1970.72 f/s\n",
      "1961800: done 9809 episodes, mean reward -200.000, speed 1865.52 f/s\n",
      "1963800: done 9819 episodes, mean reward -200.000, speed 1917.07 f/s\n",
      "1965800: done 9829 episodes, mean reward -200.000, speed 1896.59 f/s\n",
      "1968000: done 9840 episodes, mean reward -200.000, speed 2011.29 f/s\n",
      "1970000: done 9850 episodes, mean reward -200.000, speed 1886.43 f/s\n",
      "1972000: done 9860 episodes, mean reward -200.000, speed 1914.45 f/s\n",
      "1974000: done 9870 episodes, mean reward -200.000, speed 1999.87 f/s\n",
      "1976000: done 9880 episodes, mean reward -200.000, speed 1839.73 f/s\n",
      "1978000: done 9890 episodes, mean reward -200.000, speed 1852.42 f/s\n",
      "1980000: done 9900 episodes, mean reward -200.000, speed 1892.54 f/s\n",
      "1982000: done 9910 episodes, mean reward -200.000, speed 1929.80 f/s\n",
      "1984000: done 9920 episodes, mean reward -200.000, speed 1975.15 f/s\n",
      "1986000: done 9930 episodes, mean reward -200.000, speed 1880.94 f/s\n",
      "1988000: done 9940 episodes, mean reward -200.000, speed 1895.47 f/s\n",
      "1990000: done 9950 episodes, mean reward -200.000, speed 1878.10 f/s\n",
      "1992000: done 9960 episodes, mean reward -200.000, speed 1965.80 f/s\n",
      "1994000: done 9970 episodes, mean reward -200.000, speed 1889.50 f/s\n",
      "1996000: done 9980 episodes, mean reward -200.000, speed 1884.01 f/s\n",
      "1998000: done 9990 episodes, mean reward -200.000, speed 1949.68 f/s\n",
      "2000000: done 10000 episodes, mean reward -200.000, speed 1948.96 f/s\n",
      "Test done is 0.15 sec, reward -200.00, steps 200\n",
      "2001800: done 10009 episodes, mean reward -200.000, speed 1666.11 f/s\n",
      "2003800: done 10019 episodes, mean reward -200.000, speed 1937.14 f/s\n",
      "2005800: done 10029 episodes, mean reward -200.000, speed 1911.43 f/s\n",
      "2007800: done 10039 episodes, mean reward -200.000, speed 1926.50 f/s\n",
      "2009800: done 10049 episodes, mean reward -200.000, speed 1947.05 f/s\n",
      "2011800: done 10059 episodes, mean reward -200.000, speed 1889.05 f/s\n",
      "2013800: done 10069 episodes, mean reward -200.000, speed 1982.99 f/s\n",
      "2015800: done 10079 episodes, mean reward -200.000, speed 1918.04 f/s\n",
      "2017800: done 10089 episodes, mean reward -200.000, speed 1935.31 f/s\n",
      "2019800: done 10099 episodes, mean reward -200.000, speed 1904.52 f/s\n",
      "2022000: done 10110 episodes, mean reward -200.000, speed 2006.82 f/s\n",
      "2024000: done 10120 episodes, mean reward -200.000, speed 1899.04 f/s\n",
      "2026000: done 10130 episodes, mean reward -200.000, speed 1887.62 f/s\n",
      "2028000: done 10140 episodes, mean reward -200.000, speed 1896.41 f/s\n",
      "2029800: done 10149 episodes, mean reward -200.000, speed 1798.72 f/s\n",
      "2031400: done 10157 episodes, mean reward -200.000, speed 1485.48 f/s\n",
      "2033200: done 10166 episodes, mean reward -200.000, speed 1778.54 f/s\n",
      "2035000: done 10175 episodes, mean reward -200.000, speed 1716.14 f/s\n",
      "2036800: done 10184 episodes, mean reward -200.000, speed 1751.47 f/s\n",
      "2038600: done 10193 episodes, mean reward -200.000, speed 1782.04 f/s\n",
      "2040400: done 10202 episodes, mean reward -200.000, speed 1734.64 f/s\n",
      "2042400: done 10212 episodes, mean reward -200.000, speed 1782.05 f/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15420/3871398980.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRewardTracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtracker\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mstep_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_source\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mMAX_STEPS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Training Stopped after {MAX_STEPS}!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\rl-staples\\experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m                     \u001b[0mstates_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstates_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mstates_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_agent_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                     \u001b[0mg_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\rl-staples\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, states, agent_states)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mstates_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat32_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mstates_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\rl-staples\\utils.py\u001b[0m in \u001b[0;36mfloat32_preprocessor\u001b[1;34m(states)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfloat32_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnp_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mTBMeanTracker\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with utils.RewardTracker(writer) as tracker:\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        if step_idx > MAX_STEPS:\n",
    "            print(f\"Training Stopped after {MAX_STEPS}!\")\n",
    "            break\n",
    "        rewards_steps = exp_source.pop_rewards_steps()\n",
    "        if rewards_steps:\n",
    "            rewards, steps = zip(*rewards_steps)\n",
    "            writer.add_scalar(\"episode_steps\", np.mean(steps), step_idx)\n",
    "            tracker.reward(np.mean(rewards), step_idx)\n",
    "\n",
    "        if step_idx % TEST_ITERS == 0:\n",
    "            ts = time.time()\n",
    "            with torch.no_grad():\n",
    "                rewards, steps = common.test_net_discrete(\n",
    "                    net_act, test_env, device=device\n",
    "                )\n",
    "            print(\n",
    "                \"Test done is %.2f sec, reward %.2f, steps %d\"\n",
    "                % (time.time() - ts, rewards, steps)\n",
    "            )\n",
    "            writer.add_scalar(\"test_reward\", rewards, step_idx)\n",
    "            writer.add_scalar(\"test_steps\", steps, step_idx)\n",
    "            if best_reward is None or best_reward < rewards:\n",
    "                if best_reward is not None:\n",
    "                    print(\n",
    "                        \"Best reward updated: %.2f -> %.2f\" % (best_reward, rewards)\n",
    "                    )\n",
    "                    name = \"best_%+.2f_%d.dat\" % (rewards, step_idx)\n",
    "                    fname = os.path.join(save_path, name)\n",
    "                    # torch.save(net_act.state_dict(), fname)\n",
    "                best_reward = rewards\n",
    "                if best_reward > STOP_REWARD:\n",
    "                    print(\"Solved!\")\n",
    "                    break\n",
    "\n",
    "        trajectory.append(exp)\n",
    "        if len(trajectory) < TRAJECTORY_SIZE + 1:\n",
    "            continue\n",
    "\n",
    "        traj_states = [t[0].state for t in trajectory]\n",
    "        traj_actions = [t[0].action for t in trajectory]\n",
    "        traj_states_v = torch.FloatTensor(np.array(traj_states)).to(device)\n",
    "        traj_actions_v = torch.FloatTensor(np.array(traj_actions)).to(device)\n",
    "\n",
    "        traj_adv_v, traj_ref_v = calc_adv_ref(\n",
    "            trajectory, net_crt, traj_states_v, device=device\n",
    "        )\n",
    "\n",
    "        #mu_v = net_act(traj_states_v)\n",
    "        #var_v = torch.exp(net_act.logstd) ** 2\n",
    "        #old_logprob_v = common.calc_logprob(mu_v, var_v, traj_actions_v)\n",
    "        action_probs = net_act(traj_states_v)\n",
    "        dist = Categorical(action_probs)\n",
    "        old_logprob_v = dist.log_prob(torch.tensor(traj_actions))\n",
    "        # print(old_logprob_v.shape)  # [T, 2]\n",
    "\n",
    "        # normalize advantages (mean should be zero)\n",
    "        traj_adv_v = traj_adv_v - torch.mean(traj_adv_v)\n",
    "        traj_adv_v /= torch.std(traj_adv_v)\n",
    "\n",
    "        # drop last entry from the trajectory, as our adv and ref value calculated without it\n",
    "        trajectory = trajectory[:-1]\n",
    "        old_logprob_v = old_logprob_v[:-1].detach()\n",
    "\n",
    "        for epoch in range(PPO_EPOCHES):\n",
    "            for batch_ofs in range(0, len(trajectory), BATCH_SIZE):\n",
    "                batch_l = batch_ofs + BATCH_SIZE\n",
    "                states_v = traj_states_v[batch_ofs:batch_l]\n",
    "                actions_v = traj_actions_v[batch_ofs:batch_l]\n",
    "                batch_adv_v = traj_adv_v[batch_ofs:batch_l]\n",
    "                # print(batch_adv_v.shape) #[batch_size]\n",
    "                batch_adv_v = batch_adv_v.unsqueeze(-1)\n",
    "                batch_ref_v = traj_ref_v[batch_ofs:batch_l]\n",
    "                batch_old_logprob_v = old_logprob_v[batch_ofs:batch_l]\n",
    "\n",
    "                opt_crt.zero_grad()\n",
    "                value_v = net_crt(states_v)\n",
    "                loss_value = F.mse_loss(value_v.squeeze(-1), batch_ref_v)\n",
    "                loss_value.backward()\n",
    "                # if CLIP_GRAD > 0:\n",
    "                #     torch.nn.utils.clip_grad_norm_(net_crt.parameters(), CLIP_GRAD)\n",
    "                opt_crt.step()\n",
    "\n",
    "                opt_act.zero_grad()\n",
    "                #mu_v = net_act(states_v)\n",
    "                #batch_var_v = torch.exp(net_act.logstd) ** 2\n",
    "                #logprob_pi_v = common.calc_logprob(mu_v, batch_var_v, actions_v)\n",
    "                action_probs = net_act(states_v)\n",
    "                dist = Categorical(action_probs)\n",
    "                logprob_pi_v = dist.log_prob(actions_v)\n",
    "                entropy = dist.entropy() \n",
    "                \n",
    "                ratio_v = torch.exp(logprob_pi_v - batch_old_logprob_v)\n",
    "                surr_obj_v = batch_adv_v * ratio_v\n",
    "                c_ratio_v = torch.clamp(ratio_v, 1.0 - PPO_EPS, 1.0 + PPO_EPS)\n",
    "                clipped_surr_v = batch_adv_v * c_ratio_v\n",
    "                \n",
    "                loss_policy = -torch.min(surr_obj_v, clipped_surr_v).mean() - ENTROPY_BONUS* entropy.mean() \n",
    "                loss_policy.backward()\n",
    "                opt_act.step()\n",
    "\n",
    "\n",
    "        trajectory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.3721, 0.2557, 0.3722]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical(a).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
